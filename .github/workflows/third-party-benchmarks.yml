name: Third party benchmarks

on:
  workflow_dispatch:
    inputs:
      runner_label:
        description: Runner label, keep empty for default
        type: string
        default: ""
      tag:
        description: Tag for benchmark results
        type: string
        default: "test"
      benchmarks:
        description: JSON list of benchmarks to run. Leave empty to run all benchmarks.
        type: string
        default: ""
      use_pyenv_python:
        description: Use Python built with pyenv
        type: boolean
        default: false
  schedule:
    # About midnight PST (UTC-8)
    - cron: "5 10 * * *"

permissions: read-all

env:
  PYTHON_VERSION: "3.10"
  TAG: ${{ inputs.tag || (github.event_name == 'pull_request' && format('pr-{0}', github.event.number)) || (github.event_name == 'schedule' && 'ci') || 'test' }}
  HF_TOKEN: ${{ secrets.HF_TOKEN || '' }}

jobs:
  build:
    name: Triton benchmarks
    runs-on:
      - linux
      - ${{ inputs.runner_label || 'max1550' }}
    timeout-minutes: 720
    defaults:
      run:
        shell: bash -noprofile --norc -eo pipefail -c "source /opt/intel/oneapi/setvars.sh > /dev/null; source {0}"
    steps:
      - name: Print inputs
        run: |
          cat <<EOF
          ${{ toJSON(inputs) }}
          EOF

      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install Python
        if: ${{ !(inputs.use_pyenv_python || false) }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Install Python (from pyenv) ${{ inputs.python_version }}
        if: ${{ inputs.use_pyenv_python }}
        uses: ./.github/actions/setup-pyenv-python
        with:
          python-version: ${{ env.PYTHON_VERSION }}

      - name: Identify Python version
        run: |
          PYTHON_VERSION="$(python -c 'import sys; print(f"{sys.version_info[0]}.{ sys.version_info[1]}")')"
          echo "PYTHON_VERSION=$PYTHON_VERSION" | tee -a $GITHUB_ENV

      - name: Install Python build dependencies
        run: |
          pip install wheel cmake

      - name: Setup PyTorch
        uses: ./.github/actions/setup-pytorch

      - name: Setup Triton
        uses: ./.github/actions/setup-triton
        with:
          command: DEBUG=1 python setup.py bdist_wheel

      - name: Install Triton
        run: |
          pip install dist/*.whl

      - name: Install benchmark dependencies
        id: install
        run: |
          pip install transformers pandas pytest

          cd benchmarks
          pip install .
          pip install intel-pti==0.12.2
          PTI_LIBS_DIR=$(python -c "import sysconfig; print(sysconfig.get_paths()['stdlib']+'/..')")
          # the output should contain: `libpti.so`, `libpti_metrics.so.0.12.2` and `libpti_view.so.0.12.2`
          ls $PTI_LIBS_DIR
          echo "PTI_LIBS_DIR=$PTI_LIBS_DIR" >> $GITHUB_ENV

      - name: Create reports dir
        run: |
          mkdir reports
          echo "REPORTS=$PWD/reports" >> $GITHUB_ENV

      - name: Run Liger-Kernel benchmarks
        if: ${{ steps.install.outcome == 'success' && !cancelled() && (inputs.benchmarks == '' || contains(fromJson(inputs.benchmarks || '[]'), 'liger-kernel')) }}
        run: |
          source ./scripts/capture-hw-details.sh

          cd benchmarks/third_party/liger_kernels

          git clone https://github.com/linkedin/Liger-Kernel
          pip install -e Liger-Kernel

          # To remember return code, but still copy results
          RET_CODE=0
          bash ./run_benchmarks.sh || RET_CODE=$?

          cp Liger-Kernel/benchmark/data/all_benchmark_data.csv $REPORTS/liger-raw.csv
          python transform.py $REPORTS/liger-raw.csv $REPORTS/liger-report.csv --tag $TAG

          # Return the captured return code at the end
          exit "$RET_CODE"

      - name: Install SGLANG
        run: |
          git clone https://github.com/sgl-project/sglang.git
          cd sglang
          git apply ../benchmarks/third_party/sglang/sglang-fix.patch
          pip install "./python[dev_xpu]"

      # Reinstallation since SGLang installation will force overrides current PyTorch and Triton
      - name: Reinstall PyTorch
        uses: ./.github/actions/setup-pytorch

      - name: Reinstall Triton
        run: |
          pip install ./dist/*.whl

      - name: Run SGLANG attention prefill stage benchmark
        if: ${{ steps.install.outcome == 'success' && !cancelled() }}
        run: |
          export LD_LIBRARY_PATH=$PTI_LIBS_DIR:$LD_LIBRARY_PATH
          cd benchmarks/triton_kernels_benchmark
          python prefill_attention_benchmark.py --reports $REPORTS

          source ../../../scripts/capture-hw-details.sh
          python ../../triton_kernels_benchmark/build_report.py $REPORTS/sglang-prefill-attn-performance.csv $REPORTS/sglang-prefill-attn-triton-report.csv --benchmark sglang-prefill-attn --compiler triton --param_cols "B,SEQ_LENS,H_Q,H_KV,D,CAUSAL" --tflops_col Triton-TFlops --hbm_col "Triton-GB/s" --tag $TAG

      - name: Run SGLANG attention decode stage benchmark
        if: ${{ steps.install.outcome == 'success' && !cancelled() }}
        run: |
          export LD_LIBRARY_PATH=$PTI_LIBS_DIR:$LD_LIBRARY_PATH
          cd benchmarks/triton_kernels_benchmark
          python decode_attention_benchmark.py --reports $REPORTS

          source ../../../scripts/capture-hw-details.sh
          python ../../triton_kernels_benchmark/build_report.py $REPORTS/sglang-decode-attn-performance.csv $REPORTS/sglang-decode-attn-triton-report.csv --benchmark sglang-decode-attn --compiler triton --param_cols "B,SEQ_LENS,H_Q,H_KV,D" --tflops_col Triton-TFlops --hbm_col "Triton-GB/s" --tag $TAG

      - name: Run SGLANG attention append stage benchmark
        if: ${{ steps.install.outcome == 'success' && !cancelled() }}
        run: |
          export LD_LIBRARY_PATH=$PTI_LIBS_DIR:$LD_LIBRARY_PATH
          cd benchmarks/triton_kernels_benchmark
          python extended_attention_benchmark.py --reports $REPORTS

          source ../../../scripts/capture-hw-details.sh
          python ../../triton_kernels_benchmark/build_report.py $REPORTS/sglang-extended-attn-performance.csv $REPORTS/sglang-append-attn-triton-report.csv --benchmark sglang-extended-attn --compiler triton --param_cols "B,Q_LEN,PREFIX_LEN,KV_LEN,H_Q,H_KV,D" --tflops_col Triton-TFlops --hbm_col "Triton-GB/s" --tag $TAG

      - name: Run SGLANG Block FP8 GEMM benchmark
        if: ${{ steps.install.outcome == 'success' && !cancelled() }}
        run: |
          export LD_LIBRARY_PATH=$PTI_LIBS_DIR:$LD_LIBRARY_PATH
          cd benchmarks/triton_kernels_benchmark
          python block_fp8_gemm_benchmark.py --reports $REPORTS

          source ../../../scripts/capture-hw-details.sh
          python ../../triton_kernels_benchmark/build_report.py $REPORTS/sglang-fp8-gemm-performance.csv $REPORTS/sglang-fp8-gemm-triton-report.csv --benchmark sglang-block-fp8-gemm --compiler triton --param_cols "M,N,K" --tflops_col Triton-TFlops --hbm_col "Triton-GB/s" --tag $TAG

      - name: Run e2e Llama 3.1 flex attention performance benchmark
        if: ${{ steps.install.outcome == 'success' && !cancelled() && (inputs.benchmarks == '' || contains(fromJson(inputs.benchmarks || '[]'), 'llama3-1')) }}
        run: |
          source ./scripts/capture-hw-details.sh

          git clone https://github.com/huggingface/transformers.git

          cd transformers

          git checkout $(<../benchmarks/third_party/e2e-flex_attention/transformers-commit.txt)
          git apply ../benchmarks/third_party/e2e-flex_attention/transformers-patch-for-timing.diff

          git submodule sync
          git submodule update --init --recursive
          python setup.py develop

          cd ../benchmarks/third_party/e2e-flex_attention

          MODEL_NAME="meta-llama/Llama-3.1-8B"
          MAX_NEW_TOKENS=128
          INPUT_TOKENS=1024
          BATCH_SIZE=1

          python run_llm_inductor_greedy.py -m $MODEL_NAME --max-new-tokens $MAX_NEW_TOKENS --input-tokens $INPUT_TOKENS --num-warmup 2 --num-iter 7 --compile --profile | tee llm.compile.xpu.profile.log

          echo "LLM profiling log is stored into $PWD/llm.compile.xpu.profile.log"

          cp llm.compile.xpu.profile.log $REPORTS/llm.compile.xpu.profile.log
          python transform_results.py $REPORTS/llm.compile.xpu.profile.log $REPORTS/llm-triton-report.csv \
            --tag $TAG \
            --model "$MODEL_NAME" \
            --max-new-tokens $MAX_NEW_TOKENS \
            --batch-size $BATCH_SIZE

      - name: Upload benchmark reports
        if: ${{ steps.install.outcome == 'success' && !cancelled() }}
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-reports
          path: reports
