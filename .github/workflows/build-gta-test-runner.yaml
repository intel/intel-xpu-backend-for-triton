name: Build GTA test runner

on:
  workflow_dispatch:
    inputs:
      python:
        description: Python version
        default: "3.10"
      dle_installer:
        description: DLE installer file name
        default: "de3686c4-d3e1-41da-bf3b-bf5908da075c/intel-deep-learning-essentials-2025.2.1.24_offline.sh"

permissions: read-all

env:
  PYTHON_VERSION: ${{ inputs.python }}
  BENCHMARKING_METHOD: "UPSTREAM_PYTORCH_PROFILER"
  ONEAPI_DOWNLOAD_URL: "https://registrationcenter-download.intel.com/akdlm/IRC_NAS/"
  DLE_INSTALLER: ${{ inputs.dle_installer }}

jobs:

  build-installer:
    name: Build GTA test runner
    runs-on:
      - self-hosted
      - linux
      - cpu-generic
    # defaults:
    #   run:
    #     shell: bash -noprofile --norc -eo pipefail -c "source /opt/intel/oneapi/setvars.sh > /dev/null; source {0}"
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Force conda-forge channel use only
        run: |
          cat > ~/.condarc <<EOF
          channels:
            - conda-forge
          channel_priority: true
          EOF
          sudo rm -rf /home/runner/intel

      - name: Create extra files preparation env
        run: |
          source ~/miniforge3/bin/activate
          conda create -n wheels python=$PYTHON_VERSION

      - name: Get nightly wheels
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          # PYTHON_VERSION=$(python -c "import sys; print(f'{sys.version_info.major}.{sys.version_info.minor}')")
          REPO="intel/intel-xpu-backend-for-triton"
          RUN_ID=$(gh run list --workflow nightly-wheels.yml -R "$REPO" --branch main --json databaseId,conclusion | jq -r '[.[] | select(.conclusion=="success")][0].databaseId')
          echo "Run id - $RUN_ID"
          echo "RUN_ID=$RUN_ID" >> $GITHUB_ENV
          mkdir -p wheels
          WHEEL_PATTERN="wheels-pytorch-py${PYTHON_VERSION}*"
          echo "WHEEL_PATTERN=$WHEEL_PATTERN"
          TEMP_DIR=$(mktemp -d)
          gh run download $RUN_ID \
          --repo intel/intel-xpu-backend-for-triton \
          --pattern "$WHEEL_PATTERN" \
          --dir "$TEMP_DIR"
          mkdir -p wheels
          cp $TEMP_DIR/$WHEEL_PATTERN/torch-*.whl wheels/
          cp $TEMP_DIR/$WHEEL_PATTERN/triton-*.whl wheels/
          ls -R wheels

      - name: Checkout Triton commit from which nightly wheels were built
        run: |
          source ~/miniforge3/bin/activate
          conda activate wheels
          pip install packaging
          pip install wheels/*
          TRITON_COMMIT=$(python -c "import importlib.metadata; import packaging.version; print(packaging.version.Version(importlib.metadata.version('triton')).local[3:])")
          git clone --single-branch -b main --recurse-submodules https://github.com/intel/intel-xpu-backend-for-triton.git snapshot
          cd snapshot
          echo "Checking out Triton commit $TRITON_COMMIT"
          git checkout "$TRITON_COMMIT"

      # TODO: https://github.com/intel-tools/intel-xpu-backend-for-triton/issues/501
      # - name: Setup Triton kernels benchmark
      #   uses: ./.github/actions/setup-triton-kernels-benchmark
      #   with:
      #     clone_repo: false
      #     conda_env: wheels

      - name: Prepare all binary artifacts to be packaged in the installer
        env:
          GH_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          source ~/miniforge3/bin/activate

          conda activate wheels

          gh auth status

          pip install -r scripts/requirements-test.txt
          pip install defusedxml

          pip install build
          cd scripts/triton_utils
          python -m build --wheel
          pip install dist/triton_utils-*.whl
          cd ../..
          cp scripts/triton_utils/dist/*.whl wheels/

          # TODO: use pinned nightly gh run id from step "Get nightly wheels" instead of latest
          python scripts/gta_integration/constructor/abn_metadata.py

          # TODO: https://github.com/intel-tools/intel-xpu-backend-for-triton/issues/501
          # pip uninstall -y torch triton triton-kernels-benchmark triton_utils
          pip uninstall -y torch triton triton_utils
          pip freeze >requirements-offline.txt
          pip download -r requirements-offline.txt -d wheels

          # Create Triton test env wheels and repo tarballs
          # TODO: https://github.com/intel-tools/intel-xpu-backend-for-triton/issues/501
          # cp -L intel-xpu-backend-for-triton/benchmarks/dist/*.whl wheels/
          cp scripts/gta_integration/constructor/test_triton.py snapshot/scripts/test_triton.py
          cp scripts/gta_integration/constructor/test_triton_config.json snapshot/scripts/test_triton_config.json
          tar -czvf wheels.tar.gz wheels
          tar -czvf intel-xpu-backend-for-triton.tar.gz snapshot

          # Download DLE and Vulkan SDK
          wget -O intel-deep-learning-essentials_offline.sh "${ONEAPI_DOWNLOAD_URL}${DLE_INSTALLER}"
          wget -O vulkan-sdk.tar.xz https://sdk.lunarg.com/sdk/download/latest/linux/vulkan-sdk.tar.xz

      - name: Upload test reports
        uses: actions/upload-artifact@v4
        with:
          name: test_reports
          path: |
            reports-nightly/**

      - name: Upload abn_metadata
        uses: actions/upload-artifact@v4
        with:
          name: abn_metadata
          path: |
            abn_metadata.json

      - name: Build constructor installer
        run: |
          source ~/miniforge3/bin/activate
          conda create -n constructor python=$PYTHON_VERSION constructor
          conda activate constructor
          constructor scripts/gta_integration/constructor/

      - name: Upload built installer artifact and tests runner script
        uses: actions/upload-artifact@v4
        with:
          name: triton-xpu-installer
          path: |
            TritonXPU-GTA.sh
            run_triton_tests.sh

      # - name: Upload asset to GTA artifactory
      #   run: |
      #     TIMESTAMP=$(date '+%Y%m%d_%H-%M-%S')
      #     echo "TIMESTAMP=$TIMESTAMP" >> "${GITHUB_ENV}"
      #     mkdir triton-xpu-installer
      #     mv TritonXPU-GTA.sh triton-xpu-installer/
      #     mv run_triton_tests.sh triton-xpu-installer/
      #     mv abn_metadata.json triton-xpu-installer/
      #     /gta/gta-asset push gfx-ocl-abn-assets-igk/oneAPI/linux triton-xpu ${{ env.TIMESTAMP }} triton-xpu-installer \
      #         --root-url=https://gfx-assets.igk.intel.com/ \
      #         -u glados --password ${{ secrets.GLADOS_DEVTOOLS_ARTIFACTORY_TOKEN }}
