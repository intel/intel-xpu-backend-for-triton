name: Integration Tests

on:
  workflow_dispatch:
  pull_request:
    branches:
      - main


jobs:

  Runner-Preparation:
    runs-on: ubuntu-latest
    outputs:
      matrix: ${{ steps.set-matrix.outputs.matrix }}
    steps:
      - name: Prepare runner matrix
        id: set-matrix
        run: |
          if [ x"${{ github.repository }}" == x"intel/intel-xpu-backend-for-triton" ]; then
            echo '::set-output name=matrix::[["self-hosted", "ATSM"], ["self-hosted", "PVC"]]'
          else
            echo '::set-output name=matrix::["ubuntu-latest"]'
          fi
  Integration-Tests:
    needs: Runner-Preparation
    runs-on: ${{ matrix.runner }}
    strategy:
      matrix:
        runner: ${{fromJson(needs.Runner-Preparation.outputs.matrix)}}

    steps:

      - name: Create conda environment
        run: |
          source ${HOME}/env_triton.sh
          source ${HOME}/miniconda3/bin/activate triton-preci
          conda install -y astunparse numpy ninja pyyaml setuptools cmake cffi typing_extensions future six requests dataclasses mkl mkl-include
          ${HOME}/miniconda3/bin/conda install -c conda-forge pybind11 -y
        env:
          HTTP_PROXY: http://proxy.ims.intel.com:911
          HTTPS_PROXY: http://proxy.ims.intel.com:911
      
      - name: Install Dependency
        run: |
          python --version
          source ${HOME}/miniconda3/bin/activate triton-preci
          python --version
          pip install setuptools cython numpy wheel scikit-build scipy
          ln -sf /usr/lib/x86_64-linux-gnu/libstdc++.so.6 ${HOME}/miniconda3/envs/triton-preci/lib/libstdc++.so.6
          pip install psutil cpuid
          cd ${HOME}/triton-preci

          installed_llvm_git_version=$(cat ${HOME}/triton-preci/llvm/build/include/llvm/Support/VCSRevision.h | grep LLVM_REVISION | awk '{print $3}' | sed 's/"//g' || true)
          echo -e "[ INFO ] Installed llvm Hash: $installed_llvm_git_version"
          current_llvm_git_version=$(git ls-remote -- https://github.com/chengjunlu/llvm/ -b triton_debug |awk '{print $1}')
          echo -e "[ INFO ] Current llvm Hash: $current_llvm_git_version"
          if [[ "$installed_llvm_git_version" != "$current_llvm_git_version" ]];then
            echo -e "========================================================================="
            echo "llvm BUILD"
            echo -e "========================================================================="
            rm -rf ${HOME}/triton-preci/llvm
            git clone -b triton_debug https://github.com/chengjunlu/llvm/
            cd llvm
            mkdir build && cd build
            cmake ../llvm -G Ninja  -DLLVM_ENABLE_PROJECTS="mlir" -DCMAKE_BUILD_TYPE=Release -DLLVM_USE_LINKER=gold  -DLLVM_TARGETS_TO_BUILD="X86;NVPTX;AMDGPU"
            ninja all
          else
            echo -e "========================================================================="
            echo "llvm READY"
            echo -e "========================================================================="
          fi

          installed_torch_git_version=$(python -c "import torch;print(torch.version.git_version)"|| true)
          echo -e "[ INFO ] Installed Torch Hash: $installed_torch_git_version"
          current_torch_git_version=$(git ls-remote -- https://github.com/intel-innersource/frameworks.ai.pytorch.private-gpu.git -b chengjun/pytorch-2.0.1 |awk '{print $1}')
          echo -e "[ INFO ] Current Torch Hash: $current_torch_git_version"
          if [[ -z "$(pip list | grep torch.private)" || "$installed_torch_git_version" != "$current_torch_git_version" ]];then
            echo -e "========================================================================="
            echo "Private-torch BUILD"
            echo -e "========================================================================="
            rm -rf ${HOME}/triton-preci/frameworks.ai.pytorch.private-gpu
            cd ${HOME}/triton-preci
            git clone -b chengjun/pytorch-2.0.1 https://github.com/intel-innersource/frameworks.ai.pytorch.private-gpu/
            cd frameworks.ai.pytorch.private-gpu
            git submodule sync
            git submodule update --init --recursive --jobs 0
            conda install -y astunparse numpy ninja pyyaml setuptools cmake cffi typing_extensions future six requests dataclasses mkl mkl-include
            python setup.py develop
            source ${HOME}/env_triton.sh
          else
            echo -e "========================================================================="
            echo "Private-torch READY"
            echo -e "========================================================================="
          fi

          installed_IPEX_git_version=$(python -c "import torch, intel_extension_for_pytorch;print(intel_extension_for_pytorch.__ipex_gitrev__)"|| true)
          echo -e "[ INFO ] Installed IPEX Hash: $installed_IPEX_git_version"
          current_IPEX_git_version=$(git ls-remote -- https://github.com/intel-innersource/frameworks.ai.pytorch.ipex-gpu -b master |awk '{print $1}')
          current_IPEX_version=${current_IPEX_git_version: 0: 9}
          echo -e "[ INFO ] Current IPEX Hash: $current_IPEX_version"
          if [[ -z "$(pip list | grep torch.ipex-gpu)" || "$installed_IPEX_git_version" != "$current_IPEX_version" ]];then
            echo -e "========================================================================="
            echo "IPEX BUILD"
            echo -e "========================================================================="
            rm -rf ${HOME}/triton-preci/frameworks.ai.pytorch.ipex-gpu
            cd ${HOME}/triton-preci
            pip uninstall intel_extension_for_pytorch
            git clone -b master https://github.com/intel-innersource/frameworks.ai.pytorch.ipex-gpu
            cd frameworks.ai.pytorch.ipex-gpu
            git submodule sync
            git submodule update --init --recursive --jobs 0
            pip install -r requirements.txt
            USE_ONEMKL=OFF python setup.py develop
            source ${HOME}/env_triton.sh
          else
            echo -e "========================================================================="
            echo "IPEX READY"
            echo -e "========================================================================="
          fi

      - name: Build triton
        run: |
          source ${HOME}/miniconda3/bin/activate triton-preci
          source ${HOME}/env_triton.sh
          pip uninstall -y triton
          export PYBIND11_SYSPATH=${HOME}/miniconda3/envs/triton-preci/lib/python3.10/site-packages/pybind11/
          export LLVM_SYSPATH=${HOME}/triton-preci/llvm/build/
          git clone https://github.com/openai/triton triton
          cd triton
          git submodule sync
          git submodule update --init --recursive --jobs 0
          cd third_party/intel_xpu_backend
          git checkout main && git pull
          cd ../../python
          python setup.py clean
          TRITON_CODEGEN_INTEL_XPU_BACKEND=1 python setup.py develop
      
      - name: Unit test on ATSM
        if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'ATSM')}} && ${{ contains(matrix.runner, 'ATSM') }}
        run: |
          source ${HOME}/miniconda3/bin/activate triton-preci
          source ${HOME}/env_triton.sh
          export PYBIND11_SYSPATH=${HOME}/miniconda3/envs/triton-preci/lib/python3.10/site-packages/pybind11/
          export LLVM_SYSPATH=${HOME}/triton-preci/llvm/build/
          pip install pytest
          pip install pytest-xdist pytest-excel
          rm -rf ~/.triton/cache
          export TRITON_LIBDEVICE_PATH=${HOME}/actions-runner/_work/intel-xpu-backend-for-triton/triton/python/triton/third_party/xpu/ 
          git clone -b main https://github.com/intel-sandbox/triton-torch-xpu/ triton-tests
          cd ${HOME}/actions-runner/_work/intel-xpu-backend-for-triton/triton-tests/python/test/unit
          pytest -v . 2>&1 | tee ./ut_raw.log || true
          if [ ! -f "./ut_raw.log" ]; then
            echo -e "[ERROR] IPEX GPU Triton UT FAIL"
            exit 1
          fi

      - name: Unit test on PVC
        if: ${{(matrix.runner[0] == 'self-hosted') && (matrix.runner[1] == 'PVC')}} && ${{ contains(matrix.runner, 'PVC') }}
        run: |
          source ${HOME}/miniconda3/bin/activate triton-preci
          source ${HOME}/env_triton.sh
          export PYBIND11_SYSPATH=${HOME}/miniconda3/envs/triton-preci/lib/python3.10/site-packages/pybind11/
          export LLVM_SYSPATH=${HOME}/triton-preci/llvm/build/
          pip install pytest
          pip install pytest-xdist pytest-excel
          rm -rf ~/.triton/cache
          export TRITON_LIBDEVICE_PATH=${HOME}/actions-runner/_work/intel-xpu-backend-for-triton/triton/python/triton/third_party/xpu/ 
          git clone -b main https://github.com/intel-sandbox/triton-torch-xpu/ triton-tests
          cd ${HOME}/actions-runner/_work/intel-xpu-backend-for-triton/triton-tests/python/test/unit
          ZE_AFFINITY_MASK=2.0 pytest -v . 2>&1 | tee ./ut_raw.log || true
          if [ ! -f "./ut_raw.log" ]; then
            echo -e "[ERROR] IPEX GPU Triton UT FAIL"
            exit 1
          fi

      - name: Failed Log
        run: |
          echo -e "================================================================"
          echo -e "FAILED LOG"
          echo -e "================================================================"
          cd ${HOME}/actions-runner/_work/intel-xpu-backend-for-triton/triton-tests/python/test/unit
          grep "^FAILED" ut_raw.log | awk '{print $2}' |& tee ./ut_failed.log