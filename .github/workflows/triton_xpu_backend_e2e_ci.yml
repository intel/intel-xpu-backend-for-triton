name: Inductor E2E CI Tests

on:
  workflow_dispatch:
    inputs:
      torchrepo:
        description: 'torchrepo'
        required: true
        default: 'https://github.com/pytorch/pytorch.git'
      torchbranch:
        description: 'torchbranch'
        required: true
        default: 'v2.0.1'
      torchcommit:
        description: 'torchcommit'
        required: true
        default: 'e9ebda29d87ce0916ab08c06ab26fd3766a870e5'
      ipexrepo:
        description: 'ipexrepo'
        required: true
        default: 'https://github.com/intel/intel-extension-for-pytorch.git'
      ipexbranch:
        description: 'ipexbranch'
        required: true
        default: 'xpu-master'
      ipexcommit:
        description: 'ipexcommit'
        required: true
        default: '4af80f77740ed939be78eba28ae36951823f335c'
  pull_request:
    branches: [main]
  merge_group:
    branches: [main]
    types: [checks_requested]

concurrency:
  group: ${{ github.workflow }}-${{ github.event.pull_request.number || github.ref }}
  cancel-in-progress: true

jobs:

  Inductor-E2E-CI-Tests:

    runs-on: [self-hosted, PVC_E2E]

    steps:

      - name: Checkout
        uses: actions/checkout@v3

      - name: Create conda environment
        run: |
          source ${HOME}/miniconda3/bin/activate triton-preci
          conda install -y astunparse numpy ninja pyyaml setuptools cmake cffi typing_extensions future six requests dataclasses mkl mkl-include
          conda install -y -c conda-forge libstdcxx-ng

      - name: Triton source code prepare
        run: |
          source ${HOME}/miniconda3/bin/activate triton-preci
          cd ${HOME}/triton-preci
          rm -rf triton
          git clone https://github.com/openai/triton triton
          cd triton
          git submodule sync
          git submodule update --init --recursive --jobs 0
          rm -rf third_party/intel_xpu_backend/
          mkdir -p third_party/intel_xpu_backend
          cd third_party/intel_xpu_backend
          cp -r ${{ github.workspace }}/. .
          cd ../..
          git checkout `cat third_party/intel_xpu_backend/triton_hash.txt`
          triton_commit=`git rev-parse HEAD`
          echo "triton_commit: ${triton_commit}" | tee sw_info.log

      - name: Install Dependency
        run: |
          python --version
          source ${HOME}/miniconda3/bin/activate triton-preci
          python --version
          pip install setuptools cython numpy wheel scikit-build scipy
          pip install psutil cpuid
          cd ${HOME}/triton-preci
          cp triton/third_party/intel_xpu_backend/.github/scripts/env_prepare.sh .
          cp triton/third_party/intel_xpu_backend/.github/scripts/env_triton.sh ${HOME}/
          cp -r triton/third_party/intel_xpu_backend/.github/patches/ .
          bash env_prepare.sh triton-preci \
          ${{ github.event.inputs.torchrepo }} \
          ${{ github.event.inputs.torchbranch }} \
          ${{ github.event.inputs.torchcommit }} \
          ${{ github.event.inputs.ipexrepo }} \
          ${{ github.event.inputs.ipexbranch }} \
          ${{ github.event.inputs.ipexcommit }}
          source ${HOME}/env_triton.sh
          python -c "import torch;import intel_extension_for_pytorch"
          if [ ${PIPESTATUS[0]} -ne 0 ]; then
              echo -e "[ERROR] Public-torch or IPEX BUILD FAIL"
              exit 1
          fi

      - name: Build Triton
        shell: bash
        run:  |
          source ${HOME}/miniconda3/bin/activate triton-preci
          pip uninstall -y triton
          rm -rf ~/.triton/intel_llvm
          sudo update-ca-certificates --fresh
          export SSL_CERT_DIR=/etc/ssl/certs
          pip install pybind11
          cd ${HOME}/triton-preci/triton/python
          python setup.py clean
          TRITON_CODEGEN_INTEL_XPU_BACKEND=1 python setup.py bdist_wheel
          pip install dist/*.whl
          cd ${HOME}/triton-preci
          python -c "import triton"
          if [ ${PIPESTATUS[0]} -ne 0 ]; then
              echo -e "[ERROR] Triton BUILD FAIL"
              exit 1
          fi

      - name: E2E Test for triton on PVC
        run: |
          echo -e "[ INFO ] Run E2E test on Node $(hostname)"
          source ${HOME}/miniconda3/bin/activate triton-preci
          source ${HOME}/env_triton.sh
          pip install pandas
          cd ${HOME}/triton-preci
          bash set_proxy.sh
          cp ${HOME}/triton-preci/triton/third_party/intel_xpu_backend/.github/scripts/inductor_xpu_test.sh ${HOME}/triton-preci/pytorch
          cd ${HOME}/triton-preci/pytorch
          rm -rf inductor_log
          bash inductor_xpu_test.sh huggingface amp_bf16 inference accuracy xpu 0 static 2 0 & \
          bash inductor_xpu_test.sh huggingface amp_bf16 inference accuracy xpu 1 static 2 1 & \
          bash inductor_xpu_test.sh huggingface amp_fp16 inference accuracy xpu 2 static 2 0 & \
          bash inductor_xpu_test.sh huggingface amp_fp16 inference accuracy xpu 3 static 2 1 & wait
          cp ${HOME}/triton-preci/triton/sw_info.log inductor_log/

      - name: Test Results Overview
        run: |
          cd ${HOME}/triton-preci/pytorch/inductor_log/huggingface
          cd amp_bf16
          echo -e "============ Acc Check for HF amp_bf16 ============" | tee -a ./e2e_summary.log
          csv_lines_inf=$(cat inductor_huggingface_amp_bf16_inference_xpu_accuracy.csv | wc -l)
          let num_total_amp_bf16=csv_lines_inf-1
          num_passed_amp_bf16_inf=$(grep "pass" inductor_huggingface_amp_bf16_inference_xpu_accuracy.csv | wc -l)
          let num_failed_amp_bf16_inf=num_total_amp_bf16-num_passed_amp_bf16_inf
          amp_bf16_inf_acc_pass_rate=`awk 'BEGIN{printf "%.2f%%\n",('$num_passed_amp_bf16_inf'/'$num_total_amp_bf16')*100}'`
          echo "num_total_amp_bf16: $num_total_amp_bf16" | tee -a ./e2e_summary.log
          echo "num_passed_amp_bf16_inf: $num_passed_amp_bf16_inf" | tee -a ./e2e_summary.log
          echo "num_failed_amp_bf16_inf: $num_failed_amp_bf16_inf" | tee -a ./e2e_summary.log
          echo "amp_bf16_inf_acc_pass_rate: $amp_bf16_inf_acc_pass_rate" | tee -a ./e2e_summary.log

          cd ../amp_fp16
          echo -e "============ Acc Check for HF amp_fp16 ============" | tee -a ./e2e_summary.log
          csv_lines_inf=$(cat inductor_huggingface_amp_fp16_inference_xpu_accuracy.csv | wc -l)
          let num_total_amp_fp16=csv_lines_inf-1
          num_passed_amp_fp16_inf=$(grep "pass" inductor_huggingface_amp_fp16_inference_xpu_accuracy.csv | wc -l)
          let num_failed_amp_fp16_inf=num_total_amp_fp16-num_passed_amp_fp16_inf
          amp_fp16_inf_acc_pass_rate=`awk 'BEGIN{printf "%.2f%%\n",('$num_passed_amp_fp16_inf'/'$num_total_amp_fp16')*100}'`
          echo "num_total_amp_fp16: $num_total_amp_fp16" | tee -a ./e2e_summary.log
          echo "num_passed_amp_fp16_inf: $num_passed_amp_fp16_inf" | tee -a ./e2e_summary.log
          echo "num_failed_amp_fp16_inf: $num_failed_amp_fp16_inf" | tee -a ./e2e_summary.log
          echo "amp_fp16_inf_acc_pass_rate: $amp_fp16_inf_acc_pass_rate" | tee -a ./e2e_summary.log

      - name: Upload Triton Inductor E2E CI Data
        uses: actions/upload-artifact@v3
        with:
          name: Triton-Inductor-E2E-CI-Data
          path: /home/gta/triton-preci/pytorch/inductor_log/

      - name: Test Results Check
        run: |
          cd ${HOME}/triton-preci/pytorch/inductor_log/huggingface
          cd amp_bf16
          num_passed_amp_bf16_inf=$(grep "num_passed_amp_bf16_inf:" e2e_summary.log | sed -e 's/.*://;s/[^0-9.]//')
          if [ $num_passed_amp_bf16_inf -lt 45 ]; then
            echo -e "[ERROR] Inductor E2E CI test for HF amp_bf16 inference passed_num < 45"
            exit 1
          fi
          cd ../amp_fp16
          num_passed_amp_fp16_inf=$(grep "num_passed_amp_fp16_inf:" e2e_summary.log | sed -e 's/.*://;s/[^0-9.]//')
          if [ $num_passed_amp_fp16_inf -lt 45 ]; then
            echo -e "[ERROR] Inductor E2E CI test for HF amp_fp16 inference passed_num < 45"
            exit 1
          fi
