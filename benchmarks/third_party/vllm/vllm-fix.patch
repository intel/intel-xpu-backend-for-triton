diff --git a/requirements/xpu.txt b/requirements/xpu.txt
index c1dc4195b..3a154f704 100644
--- a/requirements/xpu.txt
+++ b/requirements/xpu.txt
@@ -10,9 +10,3 @@ wheel
 jinja2>=3.1.6
 datasets # for benchmark scripts
 numba == 0.61.2 # Required for N-gram speculative decoding
---extra-index-url=https://download.pytorch.org/whl/xpu
-torch==2.9.0+xpu
-torchaudio
-torchvision
-
-intel-extension-for-pytorch @ https://intel-extension-for-pytorch.s3.us-east-1.amazonaws.com/ipex_dev/xpu/intel_extension_for_pytorch-2.9.10.post0%2Bxpu-cp312-cp312-linux_x86_64.whl
diff --git a/tests/kernels/moe/test_batched_moe.py b/tests/kernels/moe/test_batched_moe.py
index dab1207d7..31e85cb6b 100644
--- a/tests/kernels/moe/test_batched_moe.py
+++ b/tests/kernels/moe/test_batched_moe.py
@@ -168,9 +168,9 @@ def test_batched_mm(
     )
 
     out_shape = (num_experts, max_tokens_per_expert, N)
-    test_output = torch.zeros(out_shape, dtype=act_dtype, device="cuda")
-    ref_output = torch.zeros(out_shape, dtype=act_dtype, device="cuda")
-    q_ref_output = torch.zeros(out_shape, dtype=act_dtype, device="cuda")
+    test_output = torch.zeros(out_shape, dtype=act_dtype, device="xpu")
+    ref_output = torch.zeros(out_shape, dtype=act_dtype, device="xpu")
+    q_ref_output = torch.zeros(out_shape, dtype=act_dtype, device="xpu")
 
     compute_tl_dtype = {
         torch.float16: tl.float16,
@@ -234,8 +234,8 @@ def test_batched_mm(
 @pytest.mark.parametrize(("m", "n", "k"), MNK_FACTORS)
 @pytest.mark.parametrize("e", NUM_EXPERTS)
 @pytest.mark.parametrize("topk", TOP_KS)
-@pytest.mark.parametrize("dtype", DTYPES)
-@pytest.mark.parametrize("per_act_token_quant", [False, True])
+@pytest.mark.parametrize("dtype", [torch.float8_e4m3fn, torch.bfloat16])
+@pytest.mark.parametrize("per_act_token_quant", [False])
 @pytest.mark.parametrize("block_shape", [None, [128, 128]])
 @pytest.mark.parametrize("input_scales", [False])
 def test_fused_moe_batched_experts(
diff --git a/tests/kernels/moe/utils.py b/tests/kernels/moe/utils.py
index f0c8c8033..784c000b7 100644
--- a/tests/kernels/moe/utils.py
+++ b/tests/kernels/moe/utils.py
@@ -3,9 +3,10 @@
 
 import torch
 
-import vllm._custom_ops as ops
 from tests.kernels.quant_utils import per_block_cast_to_int8
-from tests.kernels.quantization.nvfp4_utils import FLOAT4_E2M1_MAX, FLOAT8_E4M3_MAX
+from tests.kernels.quantization.nvfp4_utils import (FLOAT4_E2M1_MAX,
+                                                    FLOAT8_E4M3_MAX)
+from vllm import _custom_ops as ops
 from vllm.model_executor.layers.activation import SiluAndMul
 from vllm.model_executor.layers.fused_moe import fused_experts, fused_topk
 from vllm.model_executor.layers.fused_moe.config import FusedMoEQuantConfig
@@ -147,7 +148,7 @@ def make_quantized_test_activations(
     block_shape: list[int] | None = None,
     per_act_token_quant: bool = False,
 ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor | None]:
-    a = torch.randn((E, m, k), device="cuda", dtype=in_dtype) / 10
+    a = torch.randn((E, m, k), device="xpu", dtype=in_dtype) / 10
     a_q = a
     a_scale = None
 
@@ -223,7 +224,7 @@ def make_test_weight(
     block_shape: list[int] | None = None,
     per_out_ch_quant: bool = False,
 ) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor | None, torch.Tensor | None]:
-    w_16 = torch.randn((e, rows, cols), device="cuda", dtype=in_dtype) / 15
+    w_16 = torch.randn((e, rows, cols), device="xpu", dtype=in_dtype) / 15
     w_gs = None
 
     if quant_dtype is not None:
@@ -232,10 +233,14 @@ def make_test_weight(
         w_gs_l = [None] * e
         for idx in range(e):
             w_l[idx], w_s_l[idx], w_gs_l[idx] = moe_quantize_weights(
-                w_16[idx], None, quant_dtype, per_out_ch_quant, block_shape
-            )
-
-        w = torch.stack(w_l)
+                w_16[idx], None, quant_dtype, per_act_token_quant, block_shape)
+        new_shape = torch.Size((len(w_l), ) + w_l[0].shape)
+        new_w = torch.zeros(new_shape,
+                            dtype=w_l[0].dtype,
+                            device=w_l[0].device)
+        for (i, wt) in enumerate(w_l):
+            new_w[i] = wt
+        w = new_w
         w_s = torch.stack(w_s_l)
         if e > 0 and w_gs_l[0] is not None:
             w_gs = torch.stack(w_gs_l)
diff --git a/tests/kernels/utils.py b/tests/kernels/utils.py
index 72c79370d..031ef542e 100644
--- a/tests/kernels/utils.py
+++ b/tests/kernels/utils.py
@@ -906,14 +906,11 @@ def torch_experts(
                 out[mask] = tmp2 @ w2[i].transpose(0, 1)
             elif block_shape is not None:
                 # block quantized
-                assert (
-                    a_scale is not None
-                    and w1_scale is not None
-                    and w2_scale is not None
-                )
+                assert (a_scale is not None and w1_scale is not None
+                        and w2_scale is not None)
                 tmp1 = native_w8a8_block_matmul(
-                    a[mask], w1[i], a_scale[mask], w1_scale[i], block_shape, out.dtype
-                )
+                    a.to(f32)[mask], w1[i], a_scale[mask], w1_scale[i],
+                    block_shape, out.dtype)
                 if b_bias1 is not None:
                     tmp1 = tmp1 + b_bias1[i].view(1, -1).to(tmp1.dtype)
                 tmp2 = SiluAndMul()(tmp1)
@@ -933,8 +930,7 @@ def torch_experts(
                     and w2_scale is not None
                 )
                 scales = a_scale if a_scale.numel() == 1 else a_scale[mask]
-
-                tmp1 = a[mask].to(f32) * scales
+                tmp1 = a.to(f32)[mask] * scales
                 w1_dq = (w1[i].to(f32) * w1_scale[i]).transpose(0, 1)
                 tmp1 = (tmp1 @ w1_dq).to(out.dtype)
                 if b_bias1 is not None:
diff --git a/vllm/_ipex_ops.py b/vllm/_ipex_ops.py
index 95c17cb33..100c380f3 100644
--- a/vllm/_ipex_ops.py
+++ b/vllm/_ipex_ops.py
@@ -447,11 +447,23 @@ class ipex_ops:
                 "padding not supported if output passed in"
             )
             assert output.dtype == out_dtype
-        assert scale is None, "only dynamic fp8 quantization supported on XPU"
         assert not use_per_token_if_dynamic, (
-            "per token dynamic fp8 quantization not supported on XPU"
-        )
-        scale = torch.zeros(1, device=input.device, dtype=torch.float32)
-        torch.ops.torch_ipex.dynamic_scaled_fp8_quant(output, input, scale)
+            "per token dynamic fp8 quantization not supported on XPU")
+        if scale is None:
+            if use_per_token_if_dynamic:
+                scale = torch.empty((shape[0], 1),
+                                    device=input.device,
+                                    dtype=torch.float32)
+                torch.ops._C.dynamic_per_token_scaled_fp8_quant(
+                    output, input, scale, scale_ub)
+            else:
+                scale = torch.empty(1,
+                                    device=input.device,
+                                    dtype=torch.float32)
+                torch.ops.torch_ipex.dynamic_scaled_fp8_quant(
+                    output, input, scale)
+        else:
+            assert scale.numel() == 1, f"{scale.shape}"
+            torch.ops.torch_ipex.static_scaled_fp8_quant(output, input, scale)
 
         return output, scale
diff --git a/vllm/model_executor/layers/activation.py b/vllm/model_executor/layers/activation.py
index 7038d0868..1990654f7 100644
--- a/vllm/model_executor/layers/activation.py
+++ b/vllm/model_executor/layers/activation.py
@@ -71,12 +71,8 @@ class SiluAndMul(CustomOp):
 
     def __init__(self):
         super().__init__()
-        if current_platform.is_cuda_alike():
+        if current_platform.is_cuda_alike() or current_platform.is_xpu():
             self.op = torch.ops._C.silu_and_mul
-        elif current_platform.is_xpu():
-            from vllm._ipex_ops import ipex_ops
-
-            self.op = ipex_ops.silu_and_mul
         elif current_platform.is_cpu():
             self._forward_method = self.forward_native
 
@@ -94,11 +90,7 @@ class SiluAndMul(CustomOp):
         return out
 
     def forward_xpu(self, x: torch.Tensor) -> torch.Tensor:
-        d = x.shape[-1] // 2
-        output_shape = x.shape[:-1] + (d,)
-        out = torch.empty(output_shape, dtype=x.dtype, device=x.device)
-        self.op(out, x)
-        return out
+        return self.forward_cuda(x)
 
 
 @CustomOp.register("mul_and_silu")
@@ -114,12 +106,8 @@ class MulAndSilu(CustomOp):
 
     def __init__(self):
         super().__init__()
-        if current_platform.is_cuda_alike():
+        if current_platform.is_cuda_alike() or current_platform.is_xpu():
             self.op = torch.ops._C.mul_and_silu
-        elif current_platform.is_xpu():
-            from vllm._ipex_ops import ipex_ops
-
-            self.op = ipex_ops.silu_and_mul
         elif current_platform.is_cpu():
             self._forward_method = self.forward_native
 
@@ -211,24 +199,12 @@ class GeluAndMul(CustomOp):
         self.approximate = approximate
         if approximate not in ("none", "tanh"):
             raise ValueError(f"Unknown approximate mode: {approximate}")
-        if current_platform.is_cuda_alike() or current_platform.is_cpu():
+        if current_platform.is_cuda_alike() or current_platform.is_cpu(
+        ) or current_platform.is_xpu():
             if approximate == "none":
                 self.op = torch.ops._C.gelu_and_mul
             elif approximate == "tanh":
                 self.op = torch.ops._C.gelu_tanh_and_mul
-        if current_platform.is_rocm() and approximate == "tanh":
-            logger.warning_once(
-                "[ROCm] PyTorch's native GELU with tanh approximation is unstable "
-                "with torch.compile. For native implementation, fallback to 'none' "
-                "approximation. The custom kernel implementation is unaffected."
-            )
-        elif current_platform.is_xpu():
-            from vllm._ipex_ops import ipex_ops
-
-            if approximate == "none":
-                self.op = ipex_ops.gelu_and_mul
-            else:
-                self.op = ipex_ops.gelu_tanh_and_mul
 
     def forward_native(self, x: torch.Tensor) -> torch.Tensor:
         """PyTorch-native implementation equivalent to forward()."""
@@ -290,12 +266,9 @@ class SwigluOAIAndMul(CustomOp):
 class NewGELU(CustomOp):
     def __init__(self):
         super().__init__()
-        if current_platform.is_cuda_alike() or current_platform.is_cpu():
+        if current_platform.is_cuda_alike() or current_platform.is_cpu(
+        ) or current_platform.is_xpu():
             self.op = torch.ops._C.gelu_new
-        elif current_platform.is_xpu():
-            from vllm._ipex_ops import ipex_ops
-
-            self.op = ipex_ops.gelu_new
 
     def forward_native(self, x: torch.Tensor) -> torch.Tensor:
         """PyTorch-native implementation equivalent to forward()."""
@@ -315,12 +288,9 @@ class NewGELU(CustomOp):
 class FastGELU(CustomOp):
     def __init__(self):
         super().__init__()
-        if current_platform.is_cuda_alike() or current_platform.is_cpu():
+        if current_platform.is_cuda_alike() or current_platform.is_cpu(
+        ) or current_platform.is_xpu():
             self.op = torch.ops._C.gelu_fast
-        elif current_platform.is_xpu():
-            from vllm._ipex_ops import ipex_ops
-
-            self.op = ipex_ops.gelu_fast
 
     def forward_native(self, x: torch.Tensor) -> torch.Tensor:
         """PyTorch-native implementation equivalent to forward()."""
@@ -340,12 +310,9 @@ class QuickGELU(CustomOp):
     # https://github.com/huggingface/transformers/blob/main/src/transformers/activations.py#L90
     def __init__(self):
         super().__init__()
-        if current_platform.is_cuda_alike() or current_platform.is_cpu():
+        if current_platform.is_cuda_alike() or current_platform.is_cpu(
+        ) or current_platform.is_xpu():
             self.op = torch.ops._C.gelu_quick
-        elif current_platform.is_xpu():
-            from vllm._ipex_ops import ipex_ops
-
-            self.op = ipex_ops.gelu_quick
 
     def forward_native(self, x: torch.Tensor) -> torch.Tensor:
         """PyTorch-native implementation equivalent to forward()."""
diff --git a/vllm/model_executor/layers/fused_moe/fused_batched_moe.py b/vllm/model_executor/layers/fused_moe/fused_batched_moe.py
index 7fd8511e2..e99f4fb2d 100644
--- a/vllm/model_executor/layers/fused_moe/fused_batched_moe.py
+++ b/vllm/model_executor/layers/fused_moe/fused_batched_moe.py
@@ -718,10 +718,9 @@ class NaiveBatchedExperts(mk.FusedMoEPermuteExpertsUnpermute):
 
         for expert in range(num_local_experts):
             # Indexing expert_num_tokens doesn't work w/cudagraphs or inductor
-            if (
-                torch.compiler.is_compiling()
-                or torch.cuda.is_current_stream_capturing()
-            ):
+            # if (torch.compiler.is_compiling()
+            #         or torch.cuda.is_current_stream_capturing()):
+            if False:
                 num = hidden_states.shape[1]
             else:
                 num = int(expert_num_tokens[expert].item())
@@ -761,7 +760,7 @@ def batched_moe_kernel_quantize_input(
     per_act_token_quant: bool,
     block_shape: list[int] | None = None,
 ) -> tuple[torch.Tensor, torch.Tensor | None]:
-    if torch.compiler.is_compiling() or torch.cuda.is_current_stream_capturing():
+    if False:
         # Note: this does a bunch of extra work because expert_num_tokens is
         # ignored but it does support torch.compile + cudagraphs.
         hidden_dim = A.size(-1)
diff --git a/vllm/model_executor/layers/fused_moe/fused_moe.py b/vllm/model_executor/layers/fused_moe/fused_moe.py
index 0b83a3f5c..e99ad6f3a 100644
--- a/vllm/model_executor/layers/fused_moe/fused_moe.py
+++ b/vllm/model_executor/layers/fused_moe/fused_moe.py
@@ -1124,13 +1124,17 @@ def vllm_topk_softmax(
     gating_output: torch.Tensor,
     renormalize: bool,
 ) -> tuple[torch.Tensor, ...]:
-    ops.topk_softmax(
-        topk_weights,
-        topk_indices,
-        token_expert_indices,
-        gating_output,
-        renormalize,
-    )
+    if current_platform.is_xpu():
+       score = torch.softmax(gating_output, dim=-1, dtype=torch.float32)
+       topk_weights, topk_indices = torch.topk(score,
+                                               token_expert_indices.size(1))
+    else:
+       ops.topk_softmax(
+           topk_weights,
+           topk_indices,
+           token_expert_indices,
+           gating_output,
+       )
 
     return topk_weights, topk_indices
 
diff --git a/vllm/platforms/__init__.py b/vllm/platforms/__init__.py
index a45ca9882..0bc76c784 100644
--- a/vllm/platforms/__init__.py
+++ b/vllm/platforms/__init__.py
@@ -133,7 +133,6 @@ def xpu_platform_plugin() -> str | None:
     logger.debug("Checking if XPU platform is available.")
     try:
         # installed IPEX if the machine has XPUs.
-        import intel_extension_for_pytorch  # noqa: F401
         import torch
 
         if supports_xccl():
diff --git a/vllm/platforms/xpu.py b/vllm/platforms/xpu.py
index 0a0575076..182b09198 100644
--- a/vllm/platforms/xpu.py
+++ b/vllm/platforms/xpu.py
@@ -6,6 +6,7 @@ import os
 from typing import TYPE_CHECKING
 
 import torch
+import vllm_xpu_kernels._C  # noqa
 
 import vllm.envs as envs
 from vllm.attention.backends.registry import AttentionBackendEnum
@@ -206,7 +207,7 @@ class XPUPlatform(Platform):
 
     @classmethod
     def fp8_dtype(cls) -> torch.dtype:
-        return torch.float8_e5m2
+        return torch.float8_e4m3fn
 
     @classmethod
     def is_data_center_gpu(cls) -> bool:
