#########################################
"Hello, World!" and the Programming Model
#########################################

********
Glossary
********

* **GPU Kernel** (or **kernel**) is a program or function compiled separately from the main program to run on a specific GPU target. Each Triton kernel requires the ``@triton.jit`` annotation.

* **Single Program Multiple Data** (SPMD) is a programming paradigm where the same kernel (i.e., program) is replicated across multiple hardware instances. The same kernel is applied to different subsets of input data generated by dividing by the number of execution threads.

* **Thread** represents the smallest execution unit on a GPU. Threads within the same block can communicate and synchronize with each other.

* **Warp** (Nvidia notation) or **SubGroup** (Intel notation) is a one-dimensional group of threads. The GPU architecture limits the number of threads per subgroup and is typically a multiple of 32.

* **Block** (or **Thread Block** or **WorkGroup**) is a two- or three-dimensional group of threads executing the same code simultaneously on a GPU. Threads within the same block can share data through shared memory and synchronize their execution.

* **Grid** is a three-dimensional collection of thread blocks. It defines the overall structure of parallel execution on a GPU. Each grid dimension (X, Y, and Z) represents the number of thread blocks along that axis. Threads in different blocks within the same grid communicate only through global memory.

*************************
"Hello, World!" in Triton
*************************

Triton programs are single-threaded, which makes them easier to write, maintain, optimize, and debug. However, this also requires the development of automatic parallelization mechanisms. These mechanisms must generate efficient multi-threaded GPU code based on high-level specifications of blocked algorithms.

Additionally, Triton enables the distribution of kernels in a 3D logical thread space. This distribution method is commonly used to map logical threads to data, enhancing data locality. Moreover, logical threads are executed with multiple hardware threads. We will start with the simplest case with one logical thread and one kernel and then demonstrate how to expand this to achieve better performance and increase hardware utilization.

The simplest case to observe in Triton is a vector sum of two vectors. (We will use the default data type as fp32.)

Let's begin with a basic implementation. All Triton kernels require the ``@triton.jit`` annotation and accept a limited range of argument types. The supported argument types are:

    1. Pointer to data (Object with ``data_ptr()`` and ``dtype()`` - commonly, we will use ``torch.tensor`` for this purpose)
    2. Python primitive types: int, float, bool, str
    3. Constexprs (``tl.constexpr``)

.. code-block :: python

    import torch

    import triton
    import triton.language as tl


    @triton.jit
    def add_kernel(x_ptr,  # *Pointer* to first input vector.
                   y_ptr,  # *Pointer* to second input vector.
                   output_ptr,  # *Pointer* to output vector.
                   n_elements: tl.constexpr,  # Size of the vector.
                   ):

        # There are multiple 'programs' processing different data. Here, we
        # identify which program program we are in:
        pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.

        vec_offset = tl.arange(0, n_elements)

        # x_ptr + vec_offset result is a vector of pointers
        x = tl.load(x_ptr + vec_offset)
        y = tl.load(y_ptr + vec_offset)

        output = x + y

        # Write x + y back to DRAM.
        tl.store(output_ptr + vec_offset, output)

This kernel loads all available data of the whole vector with the help of ``tl.arange``. Like many other triton.language operations, ``tl.arange`` operates with ``tl.constexpr`` only.

So what is ``tl.constexpr``? It is a kernel argument specifier that indicates to the kernel that its value can be calculated during compilation. In simple words, it should be a static constant for the kernel. If its value changes, the kernel requires recompilation to produce a different executable binary.

A raw call like ``tl.load(x_ptr)`` will only load a single value. So where does ``tl.load`` load the data? It loads data into the target device's internal memory from DRAM.

Both of these calls have several additional requirements for their arguments and their values, which will be discussed later.

To run this kernel, we will need some wrapper code:

.. code-block :: python

    def add(x: torch.Tensor, y: torch.Tensor):
        # We need to preallocate the output.
        output = torch.empty_like(x)
        assert x.is_xpu and y.is_xpu and output.is_xpu
        n_elements = output.numel()

        # The SPMD launch grid denotes the number of kernel instances that run in parallel.
        # It is analogous to CUDA launch grids.
        # It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int].
        # In this case, we use a 1D grid where the size is the number of blocks:
        grid = (1, ) # same as (1, 1)

        # NOTE:
        #  - Each torch.tensor object is implicitly converted into a pointer to its first element.
        #  - `triton.jit`'ed functions can be indexed with a launch grid.
        #  - Don't forget to pass meta-parameters as keywords arguments.
        add_kernel[grid](x, y, output, n_elements=n_elements)

        return output


    # %%
    # We can now use the above function to compute the element-wise sum of two `torch.tensor` objects
    # and test its correctness:

    torch.manual_seed(0)
    size = 1024
    x = torch.rand(size, device='xpu')
    y = torch.rand(size, device='xpu')
    output_torch = x + y
    output_triton = add(x, y)
    print("Torch  output: ", output_torch)
    print("Triton output: ", output_triton)
    print(f'The maximum difference between torch and triton is '
        f'{torch.max(torch.abs(output_torch - output_triton))}')


We should pay attention to the grid in the wrapper code, which is a Tuple of 2 integers. This Tuple describes the number of kernels that will run on a GPU and describes the logical thread space for kernels in terms of blocks, similar to the CUDA programming model. A simple visualization of the kernel's grid is:

.. image :: ../pics/execution_model.png

In the current example, we are using a single kernel as we wrote it without any actual usage of pid. Grid can also be callable and return a tuple based on meta parameters. This approach will be covered later.

This code has several obvious problems:

    1. It works **only with shapes that are** :math:`2^n` as used in the load operation.
    2. It does not fully utilize the possibility to use more logical threads.

To address these issues, let's rewrite it to support any shape. We will use introduce an additional parameter ``BLOCK_SIZE`` to split (or tile) the custom shape according to the suitable granularity of the target GPU.

.. code-block :: python

    @triton.jit
    def add_kernel_fine_grained(x_ptr,  # *Pointer* to first input vector.
                                y_ptr,  # *Pointer* to second input vector.
                                output_ptr,  # *Pointer* to output vector.
                                n_elements: tl.constexpr,  # Size of the vector.
                                BLOCK_SIZE: tl.constexpr,
                                # Number of elements suitable for load/store granularity.
                            ):

        # There are multiple 'programs' processing different data. Here, we
        # identify which program we are in:
        pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.

        vec_offset = tl.arange(0, BLOCK_SIZE)
        for i in range(0, tl.cdiv(n_elements, BLOCK_SIZE)):
            block_start = i * BLOCK_SIZE
            vec_block_offset = block_start + vec_offset

            # Mask will be vector of booleans
            # Used to avoid loading rubbish into the last tile.
            mask = vec_block_offset < n_elements

            x_tile = tl.load(x_ptr + vec_block_offset, mask=mask)
            y_tile = tl.load(y_ptr + vec_block_offset, mask=mask)

            output_tile = x_tile + y_tile

            tl.store(output_ptr + vec_block_offset, output_tile, mask=mask)

Great! With the first problem resolved, we can now work with arbitrary shapes and sizes of vectors.

Let's optimize the code to utilize more threads in our grid. Additionally, we can remove the loop from the kernel and convert it into a 1D grid.

.. code-block :: python

    import torch

    import triton
    import triton.language as tl


    @triton.jit
    def add_kernel(x_ptr,  # *Pointer* to first input vector.
                   y_ptr,  # *Pointer* to second input vector.
                   output_ptr,  # *Pointer* to output vector.
                   n_elements,  # Size of the vector.
                   BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.
                   ):

        # There are multiple 'programs' processing different data. Here, we
        # identify which program we are in:
        pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.

        # This program will process inputs that are offset from the initial data.
        # For instance, if you had a vector of length 256 and block_size of 64, the programs
        # would each access the elements [0:64, 64:128, 128:192, 192:256].
        # Note that offsets is a list of pointers:
        block_start = pid * BLOCK_SIZE

        offsets = block_start + tl.arange(0, BLOCK_SIZE)
        # Create a mask to guard memory operations against out-of-bounds accesses.
        mask = offsets < n_elements

        # Load x and y from DRAM, masking out any extra elements in case the input is not a
        # multiple of the block size.
        x = tl.load(x_ptr + offsets, mask=mask)
        y = tl.load(y_ptr + offsets, mask=mask)

        output = x + y

        # Write x + y back to DRAM.
        tl.store(output_ptr + offsets, output, mask=mask)

Apply corresponding changes to the wrapper:

.. code-block :: python

    def add(x: torch.Tensor, y: torch.Tensor):
        # We need to preallocate the output.
        output = torch.empty_like(x)
        assert x.is_xpu and y.is_xpu and output.is_xpu
        n_elements = output.numel()

        # The SPMD launch grid denotes the number of kernel instances that run in parallel.
        # It is analogous to CUDA launch grids.
        #
        # Grid can be either Tuple[int], or Callable(metaparameters) -> Tuple[int].
        # In this case, we use a 1D grid where the size is the number of blocks.
        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )

        # NOTE:
        #  - Each torch.tensor object is implicitly converted into a pointer to its first element.
        #  - `triton.jit`'ed functions can be indexed with a launch grid.
        #  - Don't forget to pass meta-parameters as keywords arguments.
        add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)

        return output

    torch.manual_seed(0)
    size = 98432
    x = torch.rand(size, device='xpu')
    y = torch.rand(size, device='xpu')
    output_torch = x + y
    output_triton = add(x, y)
    print(output_torch)
    print(output_triton)
    print(f'The maximum difference between torch and triton is '
        f'{torch.max(torch.abs(output_torch - output_triton))}')

Thread blocks (in the current case, it is a vector with ``BLOCK_SIZE``) are required to execute independently: It must be possible to execute them in any order, either in parallel or in series. This independence requirement allows thread blocks to be scheduled in any order across any number of cores, enabling programmers to write code that scales with the number of cores available.

Threads within a block can cooperate by sharing data through some shared memory and by synchronizing their execution to coordinate memory accesses. Here is an example to illustrate memory organization and hierarchy:

.. image :: ../pics/memory-hierarchy.png

Let's summarize and point out what can be done in the kernel body:

    1. Several python primitive ops are available:

        * ``+``, ``-``, ``*``, ``/``, ``//``, ``%``, ``&``, ``>>``, ``<<``, ``<``, ``>``, ``|``, ``^``
        * ``range``, ``min``, ``max``, ``print``, ``float``, ``int``,
        * ``isinstance``, ``getattr``, ``len`` (need some examples for isinstance, getattr, len)
        * ``for``, ``while``, ``pass`` and literals/constants

    2. tl ops - can be called, list of them can be found `here <../python-api/triton.language.html>`_
    3. arguments of kernel can be used
    4. other jit-ed functions can be called

************************
Load and Store Semantics
************************

In general, there are several possible storage locations for data in a system that uses a GPU device. The execution unit on the GPU can perform operations with arguments that are stored in the registers of the GPU.

.. image :: ../pics/load_store.png

To manipulate the cache, there are several load/store parameters that specify cache policy and eviction algorithm:

    1. ``cache_modifier`` (str, optional) - similar to CUDA load cache parameters

        * load `parameters <https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#id82>`_

            1. ``.cg``
            2. ``.ca``

        * store `parameters <https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#id83>`_

            1. ``.wb``
            2. ``.cg``
            3. ``.cs``
            4. ``.wt``

    2. ``eviction_policy`` (str, optional)

        * ``evict_last``
        * ``evict_first``

However, these parameters are highly dependent on the GPU being used for kernel execution.

As mentioned earlier, ``tl.load(...)`` loads only a single scalar if a single pointer is passed as an argument. To load a vector or matrix, you need to create ``tl.tensor`` with one of the following `ops <../python-api/triton.language.html#creation-ops>`_:

    1. ``arange``
    2. ``cat``
    3. ``full``
    4. ``zeros``
    5. ``zeros_like``

Let's explore how to load a matrix using these primitives:

.. code-block :: python

    @triton.jit
    def matmul_kernel_dummy(
            # Pointers to matrices
            a_ptr, b_ptr, c_ptr,
            # Matrix dimensions
            M, N, K,
            # The stride variables indicate how much to increase the pointer when moving by one
            # element in a specific dimension. For example, `stride_am` represents the increment
            # in `a_ptr` to access the element one row down (A has M rows).
            stride_am, stride_ak,  #
            stride_bk, stride_bn,  #
            stride_cm, stride_cn,
            # Meta-parameters
            BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  #
    ):
        """Kernel for computing the matmul C = A x B.
        A has shape (M, K), B has shape (K, N) and C has shape (M, N)
        """
        pid_m = tl.program_id(axis=0)
        pid_n = tl.program_id(axis=1)

        # ----------------------------------------------------------
        # Create pointers for the first blocks of A and B.
        # We will advance this pointer as we move in the K direction
        # and accumulate
        # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers
        # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers
        # See above `Pointer Arithmetic` section for details
        offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
        offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
        offs_k = tl.arange(0, BLOCK_SIZE_K)
        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)
        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)

        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
        for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):
            # Load the next block of A and B, generate a mask by checking the K dimension.
            # If it is out of bounds, set it to 0.
            a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
            b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
            # We accumulate along the K dimension.
            accumulator += tl.dot(a, b)
            # Advance the ptrs to the next K block.
            a_ptrs += BLOCK_SIZE_K * stride_ak
            b_ptrs += BLOCK_SIZE_K * stride_bk
        c = accumulator.to(tl.float16)

        # -----------------------------------------------------------
        # Write back the block of the output matrix C with masks.
        offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
        offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
        c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]
        c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
        tl.store(c_ptrs, c, mask=c_mask)

    def matmul(a, b, activation=""):
        # Check constraints.
        assert a.shape[1] == b.shape[0], "Incompatible dimensions"
        assert a.is_contiguous(), "Matrix A must be contiguous"
        assert b.is_contiguous(), "Matrix B must be contiguous"
        M, K = a.shape
        K, N = b.shape
        # Allocates output.
        c = torch.empty((M, N), device=a.device, dtype=a.dtype)
        # 1D launch kernel where each block gets its own program.
        grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']), triton.cdiv(N, META['BLOCK_SIZE_N']), )
        matmul_kernel_dummy[grid](
            a, b, c,  #
            M, N, K,  #
            a.stride(0), a.stride(1),  #
            b.stride(0), b.stride(1),  #
            c.stride(0), c.stride(1),  #
        )
        return c

Working with matrices often involves manipulating pointers and assuming a specific data order. This can be inconvenient in certain scenarios and forces additional data movement that could be done during compilation. To solve this problem, use ``block_ptr``. ``block_ptr`` simplifies iterating over multi-dimensional blocks within a larger matrix along various dimensions.

.. code-block :: python

    @triton.jit
    def matmul_kernel(a_ptr, b_ptr, c_ptr,  #
                    M, N, K,  #
                    stride_am, stride_ak,  #
                    stride_bk, stride_bn,  #
                    stride_cm, stride_cn,  #
                    BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  #
                    GROUP_SIZE_M: tl.constexpr  #
                    ):
        pid = tl.program_id(axis=0)
        num_pid_n = tl.cdiv(N, BLOCK_SIZE_N)
        num_pid_m = tl.cdiv(M, BLOCK_SIZE_M)
        num_pid_in_group = GROUP_SIZE_M * num_pid_n
        group_id = pid // num_pid_in_group
        first_pid_m = group_id * GROUP_SIZE_M
        group_size_m = min(num_pid_m - first_pid_m, GROUP_SIZE_M)
        pid_m = first_pid_m + (pid % group_size_m)
        pid_n = (pid % num_pid_in_group) // group_size_m
        block_offset_m = pid_m * BLOCK_SIZE_M
        block_offset_n = pid_n * BLOCK_SIZE_N

        a_tile_ptr = tl.make_block_ptr(base=a_ptr, shape=(M, K), strides=(stride_am, stride_ak),
                                    offsets=(block_offset_m, 0), block_shape=(BLOCK_SIZE_M, BLOCK_SIZE_K), order=(1, 0))
        b_tile_ptr = tl.make_block_ptr(base=b_ptr, shape=(K, N), strides=(stride_bk, stride_bn),
                                    offsets=(0, block_offset_n), block_shape=(BLOCK_SIZE_K, BLOCK_SIZE_N), order=(0, 1))
        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)

        for k in range(0, K, BLOCK_SIZE_K):
            a = tl.load(a_tile_ptr)
            b = tl.load(b_tile_ptr)
            accumulator += tl.dot(a, b)
            a_tile_ptr = tl.advance(a_tile_ptr, [0, BLOCK_SIZE_K])
            b_tile_ptr = tl.advance(b_tile_ptr, [BLOCK_SIZE_K, 0])

        c_block_ptr = tl.make_block_ptr(base=c_ptr, shape=(M, N), strides=(stride_cm, stride_cn),
                                        offsets=(block_offset_m, block_offset_n), block_shape=(BLOCK_SIZE_M, BLOCK_SIZE_N),
                                        order=(1, 0))

        tl.store(c_block_ptr, accumulator)


    def matmul(a, b):
        # checks constraints
        assert a.shape[1] == b.shape[0], "incompatible dimensions"
        M, K = a.shape
        K, N = b.shape
        assert (K % 32 == 0), "We don't check memory-out-of-bounds with K so K must be divisible by BLOCK_SIZE_K"

        c = torch.empty((M, N), device=a.device, dtype=torch.float32)

        def grid(META):
            return (triton.cdiv(M, META['BLOCK_SIZE_M']) * triton.cdiv(N, META['BLOCK_SIZE_N']), )

        matmul_kernel[grid](
            a_ptr=a, b_ptr=b, c_ptr=c,  #
            M=M, N=N, K=K,  #
            stride_am=a.stride(0), stride_ak=a.stride(1),  #
            stride_bk=b.stride(0), stride_bn=b.stride(1),  #
            stride_cm=c.stride(0), stride_cn=c.stride(1))
        return c

If another kernel is called within the body of this kernel, the Triton compiler will attempt to optimize it by eliminating unnecessary stores to global DRAM memory. Typically, the function will be inlined. Therefor, in complex use cases where the location of the data is important and can be changed at runtime, avoid using other utility cores with load/store operations inside them.

********************
Triton Kernel Tuning
********************

Triton has a built-in feature that allows you to choose between several predefined configurations by executing them. This functionality is supported by using the ``@triton.autotune(configs=[...])`` decorator.

.. code-block :: python

    @triton.autotune(configs=[
        triton.Config(meta={'BLOCK_SIZE': 128}, num_warps=4),
        triton.Config(meta={'BLOCK_SIZE': 1024}, num_warps=8),
    ],
    key=['x_size'] # the two above configs will be evaluated anytime
                    # the value of x_size changes
    )
    @triton.jit
    def kernel(x_ptr, x_size, **META):
        BLOCK_SIZE = META['BLOCK_SIZE']

``triton.Config`` describes several parameters for a kernel. The most important and commonly used parameter is ``meta``, which is a dictionary of meta-parameters passed to the kernel as keyword arguments. Additionally, all ``tl.constexpr`` arguments of the kernel can be passed through ``meta``. Here is an example demonstrating the usage of autotuning with multiple configurations:

.. code-block :: python

    @triton.autotune(
        configs=[
            # FIXME: Once tl.dot uses DPAS put back the workload commented out.
            # triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 64, 'GROUP_SIZE_M': 8}, num_stages=3,
            #               num_warps=8),
            triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 256, 'BLOCK_SIZE_K': 32}, num_stages=4,
                        num_warps=4),
            triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4,
                        num_warps=4),
            triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=4,
                        num_warps=4),
            triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 128, 'BLOCK_SIZE_K': 32}, num_stages=4,
                        num_warps=4),
            triton.Config({'BLOCK_SIZE_M': 128, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=4,
                        num_warps=4),
            triton.Config({'BLOCK_SIZE_M': 64, 'BLOCK_SIZE_N': 32, 'BLOCK_SIZE_K': 32}, num_stages=5,
                        num_warps=2),
            triton.Config({'BLOCK_SIZE_M': 32, 'BLOCK_SIZE_N': 64, 'BLOCK_SIZE_K': 32}, num_stages=5,
                        num_warps=2),
        ],
        key=['M', 'N', 'K'],
    )
    @triton.jit
    def matmul_kernel_dummy(
            # Pointers to matrices
            a_ptr, b_ptr, c_ptr,
            # Matrix dimensions
            M, N, K,
            # The stride variables represent how much to increase the pointer by when moving by one
            # element in a specific dimension. For example, `stride_am` is how much to increase `a_ptr`
            # by to get the element one row down (A has M rows).
            stride_am, stride_ak,  #
            stride_bk, stride_bn,  #
            stride_cm, stride_cn,
            # Meta-parameters
            BLOCK_SIZE_M: tl.constexpr, BLOCK_SIZE_N: tl.constexpr, BLOCK_SIZE_K: tl.constexpr,  #
    ):
        """Kernel for computing the matmul C = A x B.
        A has shape (M, K), B has shape (K, N) and C has shape (M, N)
        """
        # -----------------------------------------------------------
        # Map program ids `pid` to the block of C it should compute.
        # This is done in a grouped ordering to promote L2 data reuse.
        # See above `L2 Cache Optimizations` section for details.
        pid_m = tl.program_id(axis=0)
        pid_n = tl.program_id(axis=1)

        # ----------------------------------------------------------
        # Create pointers for the first blocks of A and B.
        # We will advance this pointer as we move in the K direction
        # and accumulate
        # `a_ptrs` is a block of [BLOCK_SIZE_M, BLOCK_SIZE_K] pointers
        # `b_ptrs` is a block of [BLOCK_SIZE_K, BLOCK_SIZE_N] pointers
        # See above `Pointer Arithmetic` section for details
        offs_am = (pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)) % M
        offs_bn = (pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)) % N
        offs_k = tl.arange(0, BLOCK_SIZE_K)
        a_ptrs = a_ptr + (offs_am[:, None] * stride_am + offs_k[None, :] * stride_ak)
        b_ptrs = b_ptr + (offs_k[:, None] * stride_bk + offs_bn[None, :] * stride_bn)

        # -----------------------------------------------------------
        # Iterate to compute a block of the C matrix.
        # We accumulate into a `[BLOCK_SIZE_M, BLOCK_SIZE_N]` block
        # of fp32 values for higher accuracy.
        # `accumulator` will be converted back to fp16 after the loop.
        accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
        for k in range(0, tl.cdiv(K, BLOCK_SIZE_K)):
            # Load the next block of A and B, generate a mask by checking the K dimension.
            # If it is out of bounds, set it to 0.
            a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k * BLOCK_SIZE_K, other=0.0)
            b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k * BLOCK_SIZE_K, other=0.0)
            # We accumulate along the K dimension.
            accumulator += tl.dot(a, b)
            # Advance the ptrs to the next K block.
            a_ptrs += BLOCK_SIZE_K * stride_ak
            b_ptrs += BLOCK_SIZE_K * stride_bk
        c = accumulator.to(tl.float16)

        # -----------------------------------------------------------
        # Write back the block of the output matrix C with masks.
        offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
        offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
        c_ptrs = c_ptr + stride_cm * offs_cm[:, None] + stride_cn * offs_cn[None, :]
        c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
        tl.store(c_ptrs, c, mask=c_mask)


    def matmul_dummy(a, b, activation=""):
        # Check constraints.
        assert a.shape[1] == b.shape[0], "Incompatible dimensions"
        assert a.is_contiguous(), "Matrix A must be contiguous"
        assert b.is_contiguous(), "Matrix B must be contiguous"
        M, K = a.shape
        K, N = b.shape
        # Allocates output.
        c = torch.empty((M, N), device=a.device, dtype=a.dtype)
        # 1D launch kernel where each block gets its own program.
        grid = lambda META: (triton.cdiv(M, META['BLOCK_SIZE_M']), triton.cdiv(N, META['BLOCK_SIZE_N']), )
        matmul_kernel_dummy[grid](
            a, b, c,  #
            M, N, K,  #
            a.stride(0), a.stride(1),  #
            b.stride(0), b.stride(1),  #
            c.stride(0), c.stride(1),  #
        )
        return c

To choose the proper block size, you should rely on the documentation of your target GPU. Therefore, it is difficult to provide general recommendations for determining the proper sizes. The effectiveness of block size on your GPU depends on factors such as cache memory capacity, available thread count, and how the GPU runtime driver schedules kernels.

GGrid is a logical structure that enables independent execution of all kernels. Therefore, if the data accessed by these kernels intersect, the resulting performance will heavily depend on the traversal order of the grid. Triton's code can be tailored to the specifics of the GPU runtime driver that determines this traversal order. An example illustrating such a case is provided in the `matrix multiplication tutorial. <./tutorials/03-matrix-multiplication.html#>`_

Additionally, another useful utility is ``@triton.testing.perf_report``, which generates a plot for comparing Triton's implementation with a native implementation or with another Triton implementation.


.. code-block :: python

    @triton.testing.perf_report(
        triton.testing.Benchmark(
            x_names=['M', 'N', 'K'],  # Argument names to use as an x-axis for the plot
            x_vals=[128 * i for i in range(2, 33)],  # Different possible values for `x_name`
            line_arg='provider',  # The argument name whose value corresponds to a different line in the plot
            # Possible values for `line_arg`
            line_vals=['cublas', 'triton'],
            # Label name for the lines
            line_names=["cuBLAS", "Triton"],
            # Line styles
            styles=[('green', '-'), ('blue', '-')],
            ylabel="TFLOPS",  # Label name for the y-axis
            plot_name="matmul-performance",  # Name for the plot, used also as a file name for saving the plot.
            args={},
        ))
    def benchmark(M, N, K, provider):
        a = torch.randn((M, K), device='xpu', dtype=torch.float16)
        b = torch.randn((K, N), device='xpu', dtype=torch.float16)
        quantiles = [0.5, 0.2, 0.8]
        if provider == 'cublas':
            ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch.matmul(a, b), quantiles=quantiles)
        if provider == 'triton':
            ms, min_ms, max_ms = triton.testing.do_bench(lambda: matmul(a, b), quantiles=quantiles)
        perf = lambda ms: 2 * M * N * K * 1e-12 / (ms * 1e-3)
        return perf(ms), perf(max_ms), perf(min_ms)

Example of output:

.. image :: ../pics/perf-benchmark.png
