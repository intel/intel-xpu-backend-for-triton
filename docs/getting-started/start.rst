#####################################
"Hello, World!" and Programming model
#####################################

********
Glossary
********

* **GPU Kernel** (or **kernel**) is a program or function or routine that executed on target GPU device. It is some function that can be effectively executed on target GPU device in it's execution model.

* **SPMD** stands for Single Program Multiple Data paradigm wherein the same kernel (i.e., program) is replicated multiple times on a several hardware instances. The same kernel is applied to different subsets of input data generated by the division by the number of execution threads.

* **Grid** A grid is a three-dimensional collection of thread blocks. It defines the overall structure of parallel execution on the GPU. Each dimension of the grid (X, Y, and Z) represents the number of thread blocks along that axis. Threads within different blocks within the same grid can communicate with each other only through global memory.

* **Block** or **Thread Block** A thread block is a two- or three-dimensional group of threads that execute the same code simultaneously on the GPU. Threads within the same block can share data through shared memory and synchronize their execution. 

* **Warp** (Nvidia notation) or **wavefront** (AMD notation) is a one-dimensional group of threads. This is a smaller unit for extra grain. The number of threads per block is limited by the GPU architecture and is typically a multiple of 32.

* **Thread**: A thread represents the smallest unit of execution on the GPU. Threads within the same block can communicate and synchronize with each other. 


*********************
Hello World in Triton
*********************

Triton programs are single-threaded. This makes them easier to write, maintain, optimize and debug, but also begs for the existence of automatic parallelization mechanisms capable of generating efficient multi-threaded GPU code from high-level specifications of blocked algorithms.

Also Triton allows to distribute kernels in 3D logical threads space. In common such distribution used to map logical threads to data to provide more locality of data. In further logical threads will be executed with several HW threads. We will start with simplest case with 1 logical thread and 1 kernel and than will show how to expand this to achieve better performance and increase HW utilization. 

Simplest case to observe in triton is a vector sum of 2 vectors. (We will use default data type as fp32)

Let's start with dummy implementation. All Triton kernels require ``@triton.jit`` annotation and accepts limited argument types. Supported argument types are:

    1. Pointer to data (Object with ``data_ptr()`` and ``dtype()`` - in common we wll use torch.tensor for this purpose)
    2. Python primitive types: int, float, bool, str
    3. Constexprs (``tl.constexpr``)

.. code-block :: python

    import torch

    import triton
    import triton.language as tl


    @triton.jit
    def add_kernel(x_ptr,  # *Pointer* to first input vector.
                   y_ptr,  # *Pointer* to second input vector.
                   output_ptr,  # *Pointer* to output vector.
                   n_elements: tl.constexpr,  # Size of the vector.
                   ):
        
        # There are multiple 'programs' processing different data. We identify which program
        # we are here:
        pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.
        
        vec_offset = tl.arange(0, n_elements)

        # x_ptr + vec_offset result is a vector of pointers
        x = tl.load(x_ptr + vec_offset)
        y = tl.load(y_ptr + vec_offset)
        
        output = x + y
        
        # Write x + y back to DRAM.
        tl.store(output_ptr + vec_offset, output)

This kernel loads all available data of whole vector with help of ``tl.arange``. Raw call ``tl.load(x_ptr)`` will load only single value. ``tl.arange`` as many other triton.language operations operates with ``tl.constexpr`` only. 

So what is ``tl.constexpr`` is? - It is kernel argument specifier, that tells, that it's value can be calculated during it's compilation. In simple worlds - it should be static constant for kernel. If it's value changed kernel requires recompilation to produce different different executable binary. 

To run this kernel we will need some wrapper code:

.. code-block :: python 

    def add(x: torch.Tensor, y: torch.Tensor):
        # We need to preallocate the output.
        output = torch.empty_like(x)
        assert x.is_xpu and y.is_xpu and output.is_xpu
        n_elements = output.numel()
        
        # The SPMD launch grid denotes the number of kernel instances that run in parallel.
        # It is analogous to CUDA launch grids. 
        # It can be either Tuple[int], or Callable(metaparameters) -> Tuple[int].
        # In this case, we use a 1D grid where the size is the number of blocks:
        grid = (1, ) # same as (1, 1)
        
        # NOTE:
        #  - Each torch.tensor object is implicitly converted into a pointer to its first element.
        #  - `triton.jit`'ed functions can be indexed with a launch grid 
        #  - Don't forget to pass meta-parameters as keywords arguments.
        add_kernel[grid](x, y, output, n_elements=n_elements)
        
        return output


    # %%
    # We can now use the above function to compute the element-wise sum of two `torch.tensor` objects 
    # and test its correctness:

    torch.manual_seed(0)
    size = 1024
    x = torch.rand(size, device='xpu')
    y = torch.rand(size, device='xpu')
    output_torch = x + y
    output_triton = add(x, y)
    print("Torch  output: ", output_torch)
    print("Triton output: ", output_triton)
    print(f'The maximum difference between torch and triton is '
        f'{torch.max(torch.abs(output_torch - output_triton))}')


In the wrapper code we should pay attention to grid, which should be Tuple of 2 integers. It describes amount of kernels, that will run on gpu. It describes logical thread space for kernels in terms of blocks, in the same way with CUDA programming model. Simple visualization of kernel's grid is:

.. image :: ../pics/execution_model.png

In current example we are using single kernel as we wrote it without any actual usage of pid. Grid also can be callable, that return tuple, on the basis of meta parameters. This approach will be covered later.  

This code have several obvious problems:

    1. Works **only with shapes that are** :math:`2^n` as it used in load operation. 
    2. Don't fully utilizes possibility to use more logical threads. 

So, let's rewrite it to support any shape. We will use additional parameter ``BLOCK_SIZE`` to split (or tile) the custom shape according to the suitable granularity of the target GPU. 

.. code-block :: python

    @triton.jit
    def add_kernel_fine_grained(x_ptr,  # *Pointer* to first input vector.
                                y_ptr,  # *Pointer* to second input vector.
                                output_ptr,  # *Pointer* to output vector.
                                n_elements: tl.constexpr,  # Size of the vector.
                                BLOCK_SIZE: tl.constexpr,  
                                # Number of elements suitable for load/store granularity. 
                            ):
        
        # There are multiple 'programs' processing different data. We identify which program
        # we are here:
        pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.

        vec_offset = tl.arange(0, BLOCK_SIZE)
        for i in range(0, tl.cdiv(n_elements, BLOCK_SIZE)):
            block_start = i * BLOCK_SIZE
            vec_block_offset = block_start + vec_offset

            # Mask will be vector of booleans
            # Used to avoid loading rubbish into the last tile. 
            mask = vec_block_offset < n_elements
            
            x_tile = tl.load(x_ptr + vec_block_offset, mask=mask)
            y_tile = tl.load(y_ptr + vec_block_offset, mask=mask)

            output_tile = x_tile + y_tile
            
            tl.store(output_ptr + vec_block_offset, output_tile, mask=mask)

Nice! Now we resolved first problem and can work with arbitrary shape/size of vector. Let's optimize it to utilize more threads in our grid. Also we can remove loop out of the kernel and convert into 1D grid.  

.. code-block :: python

    import torch

    import triton
    import triton.language as tl


    @triton.jit
    def add_kernel(x_ptr,  # *Pointer* to first input vector.
                   y_ptr,  # *Pointer* to second input vector.
                   output_ptr,  # *Pointer* to output vector.
                   n_elements,  # Size of the vector.
                   BLOCK_SIZE: tl.constexpr,  # Number of elements each program should process.
                   ):
        
        # There are multiple 'programs' processing different data. We identify which program
        # we are here:
        pid = tl.program_id(axis=0)  # We use a 1D launch grid so axis is 0.
        
        # This program will process inputs that are offset from the initial data.
        # For instance, if you had a vector of length 256 and block_size of 64, the programs
        # would each access the elements [0:64, 64:128, 128:192, 192:256].
        # Note that offsets is a list of pointers:
        block_start = pid * BLOCK_SIZE
        
        offsets = block_start + tl.arange(0, BLOCK_SIZE)
        # Create a mask to guard memory operations against out-of-bounds accesses.
        mask = offsets < n_elements
        
        # Load x and y from DRAM, masking out any extra elements in case the input is not a
        # multiple of the block size.
        x = tl.load(x_ptr + offsets, mask=mask)
        y = tl.load(y_ptr + offsets, mask=mask)
        
        output = x + y
        
        # Write x + y back to DRAM.
        tl.store(output_ptr + offsets, output, mask=mask)

And apply corresponding changes to wrapper:

.. code-block :: python

    def add(x: torch.Tensor, y: torch.Tensor):
        # We need to preallocate the output.
        output = torch.empty_like(x)
        assert x.is_xpu and y.is_xpu and output.is_xpu
        n_elements = output.numel()
        
        # The SPMD launch grid denotes the number of kernel instances that run in parallel.
        # It is analogous to CUDA launch grids. 
        #
        # Grid can be either Tuple[int], or Callable(metaparameters) -> Tuple[int].
        # In this case, we use a 1D grid where the size is the number of blocks.
        grid = lambda meta: (triton.cdiv(n_elements, meta['BLOCK_SIZE']), )
        
        # NOTE:
        #  - Each torch.tensor object is implicitly converted into a pointer to its first element.
        #  - `triton.jit`'ed functions can be indexed with a launch grid
        #  - Don't forget to pass meta-parameters as keywords arguments.
        add_kernel[grid](x, y, output, n_elements, BLOCK_SIZE=1024)

        return output

    torch.manual_seed(0)
    size = 98432
    x = torch.rand(size, device='xpu')
    y = torch.rand(size, device='xpu')
    output_torch = x + y
    output_triton = add(x, y)
    print(output_torch)
    print(output_triton)
    print(f'The maximum difference between torch and triton is '
        f'{torch.max(torch.abs(output_torch - output_triton))}')
    
Let's sum up and point out what can be done in the kernel body:

    1. Several python primitive ops: 

        * ``+``, ``-``, ``*``, ``/``, ``//``, ``%``, ``&``, ``>>``, ``<<``, ``<``, ``>``, ``|``, ``^``
        * ``range``, ``min``, ``max``, ``print``, ``float``, ``int``, 
        * ``isinstance``, ``getattr``, ``len`` (need some examples for isinstance, getattr, len)
        * ``for``, ``while``, ``pass`` and literals/constants

    2. tl ops - can be found `here <python-api/triton.language.html>`_
    3. arguments of kernel
    4. call other jit-ed functions

.. image :: ../pics/memory-hierarchy.png

*************
Triton Syntax
*************

*********
Semantics
*********
