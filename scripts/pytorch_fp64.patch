diff --git a/benchmarks/dynamo/common.py b/benchmarks/dynamo/common.py
index 1088634ce911e8..4e70bf5bfce434 100644
--- a/benchmarks/dynamo/common.py
+++ b/benchmarks/dynamo/common.py
@@ -1598,6 +1598,12 @@ def cast_to_fp32(model, inputs):
     return cast_to(torch.float32, model, inputs)
 
 
+def cast_to_device(device, model, inputs):
+    model = model.to(device=device)
+    inputs = tree_map_only(torch.Tensor, lambda x: x.to(device=device), inputs)
+    return model, inputs
+
+
 class DummyGradScaler:
     def scale(self, loss):
         return loss
@@ -2097,10 +2103,24 @@ def record_status(accuracy_status, dynamo_start_stats):
             model_fp64 = None
             inputs_fp64 = None
             try:
-                model_fp64, inputs_fp64 = cast_to_fp64(
-                    self.deepcopy_and_maybe_parallelize(model),
-                    clone_inputs(example_inputs),
-                )
+                # Currently, XPU GEMM FP64 support is WIP. Therefore, we explicitly fallback to
+                # CPU to execute FP64 and take the result as the gold reference.
+                if current_device == "xpu":
+                    model_fp64, inputs_fp64 = cast_to_fp64(
+                        *cast_to_device(
+                            "cpu",
+                            self.deepcopy_and_maybe_parallelize(model),
+                            clone_inputs(example_inputs),
+                        )
+                    )
+                else:
+                    model_fp64, inputs_fp64 = cast_to_fp64(
+                        self.deepcopy_and_maybe_parallelize(model),
+                        clone_inputs(example_inputs),
+                    )
+
+                # current_device of init_optimizer only impacts which optimizer will be applied. It does
+                # not change any tensor internally. Hence, we leave as it is rather than passing cpu.
                 self.init_optimizer(name, current_device, model_fp64.parameters())
                 fp64_outputs = self.run_n_iterations(
                     model_fp64, inputs_fp64, self.model_iter_fn
@@ -2111,11 +2131,19 @@ def record_status(accuracy_status, dynamo_start_stats):
                     else x,
                     fp64_outputs,
                 )
-            except Exception:
+                if current_device == "xpu":
+                    fp64_outputs = tree_map_only(
+                        torch.Tensor,
+                        lambda x: x.to(device=current_device),
+                        fp64_outputs,
+                    )
+            except Exception as e:
                 log.warning(
                     "fp64 golden ref were not generated for %s. Setting accuracy check to cosine",
                     name,
                 )
+                error_msg = f"current_device={current_device}; error:{str(e)}"
+                log.warning(error_msg)
                 self.args.cosine = True
                 fp64_outputs = None
             finally:
