diff --git a/torch/_higher_order_ops/triton_kernel_wrap.py b/torch/_higher_order_ops/triton_kernel_wrap.py
index ace56135fe1..0de08671fc3 100644
--- a/torch/_higher_order_ops/triton_kernel_wrap.py
+++ b/torch/_higher_order_ops/triton_kernel_wrap.py
@@ -247,11 +247,9 @@ def generate_ttir(
         name: arg for name, arg in ordered_args.items() if not isinstance(arg, Tensor)
     }
 
-    # Build kernel signature -- doesn't include constexpr arguments.
     signature = {
         name: kernel._type_of(kernel._key_of(arg))
         for i, (name, arg) in enumerate(ordered_args.items())
-        if i not in kernel.constexprs
     }
 
     triton._C.libtriton.ir.load_dialects(context)
diff --git a/torch/_inductor/codegen/triton.py b/torch/_inductor/codegen/triton.py
index 00031a56b8d..cf08cb07c63 100644
--- a/torch/_inductor/codegen/triton.py
+++ b/torch/_inductor/codegen/triton.py
@@ -3077,7 +3077,7 @@ class TritonKernel(SIMDKernel):
         # https://github.com/pytorch/pytorch/issues/120478#issuecomment-1962822307
         # https://github.com/openai/triton/blob/231efe9ed2d200be0f69a07c298e4342b08efe3d/python/triton/runtime/jit.py#L384
         for arg_num in triton_meta["configs"][0].equal_to_1:  # type: ignore[index]
-            triton_meta["constants"][signature[arg_num].name] = 1  # type: ignore[index]
+            triton_meta["constants"][signature[arg_num[0]].name] = 1  # type: ignore[index]
 
         self.triton_meta = triton_meta
 
diff --git a/torch/_inductor/runtime/hints.py b/torch/_inductor/runtime/hints.py
index 276c01f3f42..5c633b7963b 100644
--- a/torch/_inductor/runtime/hints.py
+++ b/torch/_inductor/runtime/hints.py
@@ -48,8 +48,8 @@ if _is_triton_available():
         ):
             # Prepare the arguments for AttrsDescriptor
             kwargs = {
-                "tt.divisibility": divisible_by_16,
-                "tt.equal_to": equal_to_1,
+                "tt.divisibility": tuple([(i,) for i in divisible_by_16]),
+                "tt.equal_to": tuple([(i,) for i in equal_to_1]),
             }
 
             # Instantiate AttrsDescriptor with the prepared arguments
diff --git a/torch/_inductor/runtime/triton_heuristics.py b/torch/_inductor/runtime/triton_heuristics.py
index af8530e94d0..0d0e35d35eb 100644
--- a/torch/_inductor/runtime/triton_heuristics.py
+++ b/torch/_inductor/runtime/triton_heuristics.py
@@ -435,11 +435,20 @@ class CachingAutotuner(KernelInterface):
         else:
             triton_helpers.set_driver_to_gpu()
 
+        def fixup_signature(signature, constants):
+            signature = signature.copy()
+            for constant in constants:
+                # If it's not in the signature already, it's a constexpr
+                # argument that we need to add in
+                if constant not in signature:
+                    signature[constant] = "constexpr"
+            return signature
+
         if ASTSource:
             compile_args = (
                 ASTSource(
                     self.fn,
-                    compile_meta["signature"],
+                    fixup_signature(compile_meta["signature"], compile_meta["constants"]),
                     compile_meta["constants"],
                     compile_meta["configs"][0],
                 ),
