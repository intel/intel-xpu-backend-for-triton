diff --git a/torch/_inductor/kernel/flex/flex_decoding.py b/torch/_inductor/kernel/flex/flex_decoding.py
index 91ba941da0..a6b87212ad 100644
--- a/torch/_inductor/kernel/flex/flex_decoding.py
+++ b/torch/_inductor/kernel/flex/flex_decoding.py
@@ -6,6 +6,7 @@ from typing import Any
 import sympy
 
 import torch
+from torch._inductor.utils import can_use_tma
 from torch._inductor.virtualized import V
 
 from ... import ir
@@ -326,6 +327,9 @@ def create_flex_decoding_kernel(*args, **kwargs):
         # Set default to False
         cur_kernel_options.setdefault("USE_TMA", False)
 
+        if torch.xpu.is_available() and can_use_tma(query, key, value):
+            cur_kernel_options["USE_TMA"] = True
+
         # Add ROCm-specific parameters if they exist in the config
         for attrib in ["kpack", "matrix_instr_nonkdim", "waves_per_eu"]:
             if hasattr(conf, attrib):
diff --git a/torch/_inductor/kernel/flex/templates/flex_decode.py.jinja b/torch/_inductor/kernel/flex/templates/flex_decode.py.jinja
index 31c64055e3..a75792787a 100644
--- a/torch/_inductor/kernel/flex/templates/flex_decode.py.jinja
+++ b/torch/_inductor/kernel/flex/templates/flex_decode.py.jinja
@@ -130,10 +130,27 @@
     block_n_last_valid = tl.minimum(kv_num_blocks * SPARSE_KV_MULTIPLE, tl.maximum(tl.cdiv(KV_LEN, BLOCK_N), 1))
 
     offs_n = tl.arange(0, BLOCK_N) + off_n
+    desc_k = None
+    desc_v = None
+    {%- if USE_TMA %}
+    desc_k = tl.make_tensor_descriptor(
+        base=K,
+        shape=[KV_LEN, QK_HEAD_DIM],
+        strides=[stride_kn, 1],
+        block_shape=[BLOCK_N, QK_HEAD_DIM_ROUNDED],
+    )
+
+    desc_v = tl.make_tensor_descriptor(
+        base=V,
+        shape=[KV_LEN, V_HEAD_DIM],
+        strides=[stride_vn, 1],
+        block_shape=[BLOCK_N, V_HEAD_DIM_ROUNDED],
+    )
+    {%- endif %}
 
     acc, l_i, m_i = forward_inner(
         {{gen_argdefs()}},
-        q, K, V, None, None, Q_LEN, KV_LEN,
+        q, K, V, desc_k, desc_v, Q_LEN, KV_LEN,
         # accumulatd values
         acc, l_i, m_i,
         #offsets
@@ -166,9 +183,27 @@
 
         offs_n = tl.arange(0, BLOCK_N) + off_n
 
+        desc_k = None
+        desc_v = None
+        {%- if USE_TMA %}
+        desc_k = tl.make_tensor_descriptor(
+            base=K,
+            shape=[KV_LEN, QK_HEAD_DIM],
+            strides=[stride_kn, 1],
+            block_shape=[BLOCK_N, QK_HEAD_DIM_ROUNDED],
+        )
+
+        desc_v = tl.make_tensor_descriptor(
+            base=V,
+            shape=[KV_LEN, V_HEAD_DIM],
+            strides=[stride_vn, 1],
+            block_shape=[BLOCK_N, V_HEAD_DIM_ROUNDED],
+        )
+        {%- endif %}
+
         acc, l_i, m_i = forward_inner(
             {{gen_argdefs()}},
-            q, K, V, None, None, Q_LEN, KV_LEN,
+            q, K, V, desc_k, desc_v, Q_LEN, KV_LEN,
             # accumulatd values
             acc, l_i, m_i,
             #offsets
