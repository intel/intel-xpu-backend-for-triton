diff --git a/.github/workflows/_xpu-test.yml b/.github/workflows/_xpu-test.yml
index d27325b8a63dc..64779b5810524 100644
--- a/.github/workflows/_xpu-test.yml
+++ b/.github/workflows/_xpu-test.yml
@@ -29,7 +29,7 @@ on:
       timeout-minutes:
         required: false
         type: number
-        default: 300
+        default: 600
         description: |
           Set the maximum (in minutes) how long the workflow should take to finish
       tests-to-include:
diff --git a/test/inductor/test_flex_attention.py b/test/inductor/test_flex_attention.py
index 7a2f9ecdeae8b..eee1f0c1ffcec 100644
--- a/test/inductor/test_flex_attention.py
+++ b/test/inductor/test_flex_attention.py
@@ -217,11 +217,7 @@ def __contains__(self, item):
         return item in self.items
 
 
-DEVICE_SUPPORTS_BACKWARDS = SubstringSet(
-    [
-        "cuda",
-    ]
-)
+DEVICE_SUPPORTS_BACKWARDS = SubstringSet(["cuda", "xpu"])
 
 device_configs["cuda"] = DeviceConfig(
     dtypes=(
@@ -959,7 +955,6 @@ def run_dynamic_test(
         q1_gold, k1_gold, v1_gold = query_key_value_clones(q1, k1, v1, torch.float64)
         ref_out1 = sdpa_partial1(q1_ref, k1_ref, v1_ref)
         golden_out1 = sdpa_partial1(q1_gold, k1_gold, v1_gold)
-
         if requires_grad:
             backward_grad1 = torch.randn((B, H, S, D), dtype=dtype, device=device)
             golden_out1.backward(backward_grad1.to(torch.float64))
diff --git a/torch/_inductor/kernel/flex/flex_attention.py b/torch/_inductor/kernel/flex/flex_attention.py
index 1a72e279aab79..5a4ed7190ef46 100644
--- a/torch/_inductor/kernel/flex/flex_attention.py
+++ b/torch/_inductor/kernel/flex/flex_attention.py
@@ -800,6 +800,13 @@ def flex_attention_backward(*args, **kwargs):
                 "num_buffers_warp_spec", num_buffers_warp_spec
             )
 
+        USE_TMA_DEFAULT = False
+        # The shape dtype of tensor desc is i32
+        if V.graph.sizevars.statically_known_true(
+            seq_len_q > 2**31 - 1
+        ) or V.graph.sizevars.statically_known_true(seq_len_kv > 2**31 - 1):
+            USE_TMA_DEFAULT = False
+        cur_kernel_options.setdefault("USE_TMA", USE_TMA_DEFAULT)
         cur_kernel_options.setdefault("BLOCK_M1", conf.block_m1)
         cur_kernel_options.setdefault("BLOCK_N1", conf.block_n1)
         cur_kernel_options.setdefault("BLOCK_M2", conf.block_m2)
diff --git a/torch/_inductor/kernel/flex/templates/flex_backwards.py.jinja b/torch/_inductor/kernel/flex/templates/flex_backwards.py.jinja
index 3467d84475d0c..11e1ed31d3732 100644
--- a/torch/_inductor/kernel/flex/templates/flex_backwards.py.jinja
+++ b/torch/_inductor/kernel/flex/templates/flex_backwards.py.jinja
@@ -111,16 +111,53 @@
         LSE2 = LSE + off_chz2
         DELTA2 = DELTA + off_chz2
 
-        # dq = tl.zeros([BLOCK_M2, QK_HEAD_DIM], dtype=tl.float32)
         dq = tl.zeros([BLOCK_M2, QK_HEAD_DIM_ROUNDED], dtype=tl.float32)
 
         start_m2 = start_m2_block * BLOCK_M2
         offs_m2 = start_m2 + tl.arange(0, BLOCK_M2)
 
+        desc_q = None
+        desc_do = None
+        desc_k = None
+        desc_v = None
+        {%- if USE_TMA %}
+        desc_q = tl.make_tensor_descriptor(
+            base=Q2,
+            shape=(Q_LEN, QK_HEAD_DIM),
+            strides=(stride_qm, stride_qd),
+            block_shape=[BLOCK_M2, QK_HEAD_DIM_ROUNDED],
+        )
+        desc_do = tl.make_tensor_descriptor(
+            base=DO2,
+            shape=(Q_LEN, V_HEAD_DIM),
+            strides=(stride_dom, stride_dod),
+            block_shape=[BLOCK_M2, V_HEAD_DIM_ROUNDED],
+        )
+        desc_k = tl.make_tensor_descriptor(
+            base=K,
+            shape=(KV_LEN, QK_HEAD_DIM),
+            strides=(stride_kn, stride_kd),
+            block_shape=[BLOCK_N2, QK_HEAD_DIM_ROUNDED],
+        )
+        desc_v = tl.make_tensor_descriptor(
+            base=V,
+            shape=(KV_LEN, V_HEAD_DIM),
+            strides=(stride_vn, stride_vd),
+            block_shape=[BLOCK_N2, V_HEAD_DIM_ROUNDED],
+        )
+        q = tl.load_tensor_descriptor(
+            desc_q,
+            [start_m2.to(tl.int32), 0],
+        )
+        do = tl.load_tensor_descriptor(
+            desc_do,
+            [start_m2.to(tl.int32), 0],
+        )
+        {%- else %}
         # load Q and do: they stay in SRAM throughout the inner loop.
         q = load_checked_2d(Q2, offs_m2, offs_k, stride_qm, stride_qd, IS_DIVISIBLE, SAFE_HEAD_DIM, Q_LEN, QK_HEAD_DIM)
         do = load_checked_2d(DO2, offs_m2, offs_v, stride_dom, stride_dod, IS_DIVISIBLE, SAFE_HEAD_DIM, Q_LEN, V_HEAD_DIM)
-
+        {%- endif %}
         if PRESCALE_QK:
             q = (q * SM_SCALE * RCP_LN2).to(MATMUL_PRECISION)
 
@@ -139,12 +176,11 @@
         kv_start = tl.load(kv_indices) * SPARSE_KV_BLOCK_SIZE # first kv block we're loading
         sparse_kv_num_blocks = tl.load(KV_NUM_BLKS + sparse_kv_num_blks_offset)
 
-        offs_n2 = kv_start + tl.arange(0, BLOCK_N2)
         dq = bwd_dq_inner(
             {{gen_argdefs()}},
-            K, V,
+            K, V, desc_k, desc_v, kv_start, start_m2,
             dq, q, do, Di, lse,
-            off_zq, off_hq2, offs_m2, offs_n2,
+            off_zq, off_hq2,
             stride_kn, stride_kd, stride_vn, stride_vd,
             kv_indices, sparse_kv_num_blocks,
             MATMUL_PRECISION,
@@ -161,9 +197,9 @@
             offs_n2 = kv_start + tl.arange(0, BLOCK_N2)
             dq = bwd_dq_inner(
                 {{gen_argdefs()}},
-                K, V,
+                K, V, desc_k, desc_v, kv_start, start_m2,
                 dq, q, do, Di, lse,
-                off_zq, off_hq2, offs_m2, offs_n2,
+                off_zq, off_hq2,
                 stride_kn, stride_kd, stride_vn, stride_vd,
                 kv_indices, sparse_kv_num_blocks,
                 MATMUL_PRECISION,
@@ -195,10 +231,32 @@
         start_n1 = pid * BLOCK_N1
         offs_n1 = start_n1 + tl.arange(0, BLOCK_N1)
 
+        {%- if USE_TMA %}
+        desc_k = tl.make_tensor_descriptor(
+            base=K,
+            shape=(KV_LEN, QK_HEAD_DIM),
+            strides=(stride_kn, stride_kd),
+            block_shape=[BLOCK_N1, QK_HEAD_DIM_ROUNDED],
+        )
+        desc_v = tl.make_tensor_descriptor(
+            base=V,
+            shape=(KV_LEN, V_HEAD_DIM),
+            strides=(stride_vn, stride_vd),
+            block_shape=[BLOCK_N1, V_HEAD_DIM_ROUNDED],
+        )
+        k = tl.load_tensor_descriptor(
+            desc_k,
+            [start_n1.to(tl.int32), 0],
+        )
+        v = tl.load_tensor_descriptor(
+            desc_v,
+            [start_n1.to(tl.int32), 0],
+        )
+        {%- else %}
         # load K and V: they stay in SRAM throughout the inner loop.
         k = load_checked_2d(K, offs_n1, offs_k, stride_kn, stride_kd, IS_DIVISIBLE, SAFE_HEAD_DIM, KV_LEN, QK_HEAD_DIM)
         v = load_checked_2d(V, offs_n1, offs_v, stride_vn, stride_vd, IS_DIVISIBLE, SAFE_HEAD_DIM, KV_LEN, V_HEAD_DIM)
-
+        {%- endif %}
         if PRESCALE_QK:
             k = (k * SM_SCALE * RCP_LN2).to(MATMUL_PRECISION)
 
@@ -231,9 +289,26 @@
             sparse_q_num_blocks = tl.load(Q_NUM_BLKS + sparse_q_num_blks_offset)
 
             offs_m1 = q_start + tl.arange(0, BLOCK_M1)
+            desc_q = None
+            desc_do = None
+            {%- if USE_TMA %}
+            desc_q = tl.make_tensor_descriptor(
+                base=Q1,
+                shape=(Q_LEN, QK_HEAD_DIM),
+                strides=(stride_qm, stride_qd),
+                block_shape=[BLOCK_M1, QK_HEAD_DIM_ROUNDED],
+            )
+            desc_do = tl.make_tensor_descriptor(
+                base=DO1,
+                shape=(Q_LEN, V_HEAD_DIM),
+                strides=(stride_dom, stride_dod),
+                block_shape=[BLOCK_M1, V_HEAD_DIM_ROUNDED],
+            )
+            {%- endif %}
             dk, dv = bwd_dkdv_inner(
                 {{gen_argdefs()}},
                 Q1, DO1, DELTA1, LSE1,
+                desc_q, desc_do, q_start,
                 dk, dv, k, v,
                 off_zq, off_hq1, offs_n1, offs_m1,
                 stride_qm, stride_qd, stride_dom, stride_dod,
@@ -254,6 +329,7 @@
                 dk, dv = bwd_dkdv_inner(
                     {{gen_argdefs()}},
                     Q1, DO1, DELTA1, LSE1,
+                    desc_q, desc_do, q_start,
                     dk, dv, k, v,
                     off_zq, off_hq1, offs_n1, offs_m1,
                     stride_qm, stride_qd, stride_dom, stride_dod,
@@ -289,14 +365,15 @@
 @triton.jit
 def bwd_dq_inner(
     {{gen_argdefs()}},
-    K, V,  # pointers
+    K, V, desc_k, desc_v, kv_start, start_m2,
     dq, q, do, Di, lse,
-    off_z, off_hq, offs_m2, offs_n2,
+    off_z, off_hq,
     stride_kn, stride_kd, stride_vn, stride_vd,
     kv_indices, sparse_kv_num_blocks,
     MATMUL_PRECISION,
     IS_FULL_BLOCKS,
 ):
+
     {{gen_defines() | indent_except_first(1) }}
     SPARSE_KV_MULTIPLE: tl.constexpr = (SPARSE_KV_BLOCK_SIZE // BLOCK_N2)
     RCP_LN2: tl.constexpr = 1.44269504
@@ -306,18 +383,18 @@ def bwd_dq_inner(
     offs_k = tl.arange(0, QK_HEAD_DIM_ROUNDED)
     offs_v = tl.arange(0, V_HEAD_DIM_ROUNDED)
 
-    kT_ptrs = K + offs_n2[None, :] * stride_kn + offs_k[:, None] * stride_kd
-    vT_ptrs = V + offs_n2[None, :] * stride_vn + offs_v[:, None] * stride_vd
     # BLOCK_M2 must be a multiple of BLOCK_N2, otherwise the code wouldn't work.
     tl.static_assert(BLOCK_M2 % BLOCK_N2 == 0)
 
     hi = tl.minimum(sparse_kv_num_blocks * SPARSE_KV_MULTIPLE, tl.maximum(tl.cdiv(KV_LEN, BLOCK_N2), 1))
 
+    kv_offset = 0
     for start_n in range(0, hi):
         dq = bwd_dq_block_mn(
             {{gen_argdefs()}},
-            dq, q, kT_ptrs, vT_ptrs, do, Di, lse, Q_LEN, KV_LEN,
-            off_z, off_hq, offs_m2, offs_n2, offs_k, offs_v,
+            dq, q, K, V, desc_k, desc_v, kv_start, kv_offset, start_m2,
+            do, Di, lse, Q_LEN, KV_LEN,
+            off_z, off_hq, offs_k, offs_v,
             stride_kn, stride_kd, stride_vn, stride_vd,
             kv_indices, sparse_kv_num_blocks,
             MATMUL_PRECISION, RCP_LN2,
@@ -330,10 +407,7 @@ def bwd_dq_inner(
             SPARSE_KV_BLOCK_SIZE, SPARSE_KV_MULTIPLE, BLOCK_N2, BLOCKS_ARE_CONTIGUOUS
         )
 
-        kT_ptrs += offset * stride_kn
-        vT_ptrs += offset * stride_vn
-
-        offs_n2 += offset
+        kv_offset += offset
 
     return dq
 
@@ -341,8 +415,9 @@ def bwd_dq_inner(
 @triton.jit
 def bwd_dq_block_mn(
     {{gen_argdefs()}},
-    dq, q, kT_ptrs, vT_ptrs, do, Di, lse, Q_LEN, KV_LEN,
-    off_z, off_hq, offs_m2, offs_n2, offs_k, offs_v,
+    dq, q, K, V, desc_k, desc_v, kv_start, kv_offset, start_m2,
+    do, Di, lse, Q_LEN, KV_LEN,
+    off_z, off_hq, offs_k, offs_v,
     stride_kn, stride_kd, stride_vn, stride_vd,
     kv_indices, sparse_kv_num_blocks,
     MATMUL_PRECISION, RCP_LN2,
@@ -350,8 +425,20 @@ def bwd_dq_block_mn(
 ):
     {{gen_defines() | indent_except_first(1)}}
 
+    offs_n2 = kv_start + kv_offset + tl.arange(0, BLOCK_N2)
+    offs_m2 = start_m2  + tl.arange(0, BLOCK_M2)
+    {%- if USE_TMA %}
+    k = tl.load_tensor_descriptor(
+        desc_k,
+        [(kv_start + kv_offset).to(tl.int32), 0],
+    )
+    kT = tl.trans(k)
+    {%- else %}
+    kT_ptrs = K + offs_n2[None, :] * stride_kn + offs_k[:, None] * stride_kd
+    vT_ptrs = V + offs_n2[None, :] * stride_vn + offs_v[:, None] * stride_vd
     # NB reversed order to since K is transposed
     kT = load_checked_2d(kT_ptrs, offs_k, offs_n2, None, None, SAFE_HEAD_DIM, IS_DIVISIBLE, QK_HEAD_DIM, KV_LEN)
+    {%- endif %}
     qk = tl.dot(q, kT, input_precision=FLOAT32_PRECISION)
     if not PRESCALE_QK:
         qk *= SM_SCALE
@@ -401,8 +488,15 @@ def bwd_dq_block_mn(
     p = tl.math.exp2(post_mod_scores - lse)
     # Compute dP and dS.
     # NB reversed order to since V is transposed
+    {%- if USE_TMA %}
+    v = tl.load_tensor_descriptor(
+        desc_v,
+        [(kv_start + kv_offset).to(tl.int32), 0],
+    )
+    vT = tl.trans(v)
+    {%- else %}
     vT = load_checked_2d(vT_ptrs, offs_v, offs_n2, None, None, SAFE_HEAD_DIM, IS_DIVISIBLE, V_HEAD_DIM, KV_LEN)
-
+    {%- endif %}
     dp = tl.dot(do, vT, input_precision=FLOAT32_PRECISION)
     ds = p * (dp - Di[:, None])
     # ~~~~~~~~~~~~~~~~~~~ Apply joint modification  ~~~~~~~~~~~~~~~~~~~
@@ -452,6 +546,7 @@ def bwd_dq_block_mn(
 def bwd_dkdv_inner(
     {{gen_argdefs()}},
     Q, DO, DELTA, LSE, # pointers
+    desc_q, desc_do, q_start,
     dk, dv, k, v,
     off_z, off_hq, offs_n1, offs_m1,
     stride_qm, stride_qd, stride_dom, stride_dod,
@@ -468,8 +563,8 @@ def bwd_dkdv_inner(
     offs_k = tl.arange(0, QK_HEAD_DIM_ROUNDED)
     offs_v = tl.arange(0, V_HEAD_DIM_ROUNDED)
 
-    qT_ptrs = Q + offs_m1[None, :] * stride_qm + offs_k[:, None] * stride_qd
-    do_ptrs = DO + offs_m1[:, None] * stride_dom + offs_v[None, :] * stride_dod
+    #qT_ptrs = Q + offs_m1[None, :] * stride_qm + offs_k[:, None] * stride_qd
+    #do_ptrs = DO + offs_m1[:, None] * stride_dom + offs_v[None, :] * stride_dod
     # BLOCK_N1 must be a multiple of BLOCK_M1, otherwise the code wouldn't work.
     tl.static_assert(BLOCK_N1 % BLOCK_M1 == 0)
 
@@ -477,10 +572,12 @@ def bwd_dkdv_inner(
     # SPARSE_BLOCK_SIZE (i.e. no block-mask!)
     hi = tl.minimum(sparse_q_num_blocks * SPARSE_Q_MULTIPLE, tl.maximum(tl.cdiv(Q_LEN, BLOCK_M1), 1))
 
+    offset_block = 0
     for start_m in range(0, hi):
         dk, dv = bwd_dkdv_block_mn(
             {{gen_argdefs()}},
-            dk, dv, qT_ptrs, k, v, do_ptrs, DELTA, LSE, Q_LEN, KV_LEN,
+            dk, dv, Q, k, v, DO, DELTA, LSE,
+            desc_q, desc_do, q_start, offset_block, Q_LEN, KV_LEN,
             off_z, off_hq, offs_n1, offs_m1, offs_k, offs_v,
             stride_qm, stride_qd, stride_dom, stride_dod,
             q_indices, sparse_q_num_blocks,
@@ -492,10 +589,8 @@ def bwd_dkdv_inner(
             start_m, q_indices, sparse_q_num_blocks,
             SPARSE_Q_BLOCK_SIZE, SPARSE_Q_MULTIPLE, BLOCK_M1, BLOCKS_ARE_CONTIGUOUS
         )
-
-        qT_ptrs += offset * stride_qm
-        do_ptrs += offset * stride_dom
         offs_m1 += offset
+        offset_block += offset
 
     return dk, dv
 
@@ -503,7 +598,8 @@ def bwd_dkdv_inner(
 @triton.jit
 def bwd_dkdv_block_mn(
     {{gen_argdefs()}},
-    dk, dv, qT_ptrs, k, v, do_ptrs, DELTA, LSE, Q_LEN, KV_LEN,
+    dk, dv, Q, k, v, DO, DELTA, LSE,
+    desc_q, desc_do, q_start, offset, Q_LEN, KV_LEN,
     off_z, off_hq, offs_n1, offs_m1, offs_k, offs_v,
     stride_qm, stride_qd, stride_dom, stride_dod,
     q_indices, sparse_q_num_blocks,
@@ -512,8 +608,18 @@ def bwd_dkdv_block_mn(
 ):
     {{gen_defines() | indent_except_first(1) }}
 
+    {%- if USE_TMA %}
+    q = tl.load_tensor_descriptor(
+        desc_q,
+        [(q_start + offset).to(tl.int32), 0],
+    )
+    qT = tl.trans(q)
+    {%- else %}
+    qT_ptrs = Q + offs_m1[None, :] * stride_qm + offs_k[:, None] * stride_qd
+    do_ptrs = DO + offs_m1[:, None] * stride_dom + offs_v[None, :] * stride_dod
     # NB reversed order since Q is transposed
     qT = load_checked_2d(qT_ptrs, offs_k, offs_m1, None, None, SAFE_HEAD_DIM, IS_DIVISIBLE, QK_HEAD_DIM, Q_LEN)
+    {%- endif %}
     # Load LSE before computing qk to reduce pipeline stall.
     if IS_DIVISIBLE:
         lse = tl.load(LSE + offs_m1)
@@ -564,7 +670,14 @@ def bwd_dkdv_block_mn(
     if not PRESCALE_QK:
         post_mod_scores *= RCP_LN2
     pT = tl.math.exp2(post_mod_scores - lse[None, :])
+    {%- if USE_TMA %}
+    do = tl.load_tensor_descriptor(
+        desc_do,
+        [(q_start + offset).to(tl.int32), 0],
+    )
+    {%- else %}
     do = load_checked_2d(do_ptrs, offs_m1, offs_v, None, None, IS_DIVISIBLE, SAFE_HEAD_DIM, Q_LEN, V_HEAD_DIM)
+    {%- endif %}
     # Compute dV.
     ppT = pT
     dv += tl.dot(ppT.to(MATMUL_PRECISION), do, input_precision=FLOAT32_PRECISION)
diff --git a/torch/_inductor/template_heuristics/triton.py b/torch/_inductor/template_heuristics/triton.py
index 9df8d114ef67b..49ce54ad7f8b1 100644
--- a/torch/_inductor/template_heuristics/triton.py
+++ b/torch/_inductor/template_heuristics/triton.py
@@ -1510,7 +1510,7 @@ def get_flex_attn_bwd_configs(
             if head_dim == 64:
                 default_config = FlexBwDConfig(64, 64, 64, 64, 1, 8)
             elif head_dim == 128:
-                default_config = FlexBwDConfig(64, 128, 64, 128, 1, 8)
+                default_config = FlexBwDConfig(64, 64, 64, 64, 1, 8)
             else:
                 default_config = FlexBwDConfig(64, 64, 64, 64, 1, 8)
         else:  # modest hardware or extremely large head_dim
